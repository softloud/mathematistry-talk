
@misc{centre_for_open_science_preregistration_2020,
	title = {Preregistration},
	url = {https://cos.io/prereg/},
	urldate = {2019-12-28},
	author = {Centre for Open Science},
	year = {2020}
}

@article{karp_robodebt:_2019,
	chapter = {Australia news},
	title = {Robodebt: the federal court ruling and what it means for targeted welfare recipients},
	issn = {0261-3077},
	shorttitle = {Robodebt},
	url = {https://www.theguardian.com/australia-news/2019/nov/28/robodebt-the-federal-court-ruling-and-what-it-means-for-targeted-welfare-recipients},
	abstract = {The Coalition has been forced to dismantle a central plank of its controversial debt recovery program},
	language = {en-GB},
	urldate = {2020-02-02},
	journal = {The Guardian},
	author = {Karp, Paul},
	month = nov,
	year = {2019},
	keywords = {Australia news, Australian politics, Centrelink, Centrelink debt recovery, Welfare}
}

@article{mcilroy201720,
	title = {20,000 people sent centrelink “robo-debt” notices found to owe less or nothing},
	volume = {13},
	journal = {Canberra Times},
	author = {McIlroy, T},
	year = {2017}
}

@book{galway_flute_1990,
	title = {Flute},
	isbn = {978-1-871082-13-5},
	abstract = {Who better to write an authoritative yet fascinating introduction to flute-playing than James Galway whose glittering career extends from the principal flute in the Berlin Philharmonic to the top of the international pop charts? He starts with the history of the flute -- believed to be the first and in its simplest form, the most basic of man's many melodic instruments: only singers have less paraphernalia between them and their listeners. You just put your lips to the flute and blow. Galway entrances with his tale of the flute's evolution from the basic recorder to the complex, beautiful instrument we know today. The author's unique advice and experience is brought to bear on the problems and techniques of learning, practising and playing -- in solo, ensemble, at home, in concert and in the recording studio. The flautist will find the specific advice Galway gives invaluable, while the non-flute player will gain an insight into the way the lovely sounds of the flute are produced. Both will be enthralled by detailed analyses of the author's favourite pieces, while he gives due attention to the whole gamut of the flute repertoire.},
	language = {en},
	publisher = {Kahn \& Averill},
	author = {Galway, James},
	year = {1990},
	note = {Google-Books-ID: a3QDAAAACAAJ},
	keywords = {Music / Musical Instruments / Woodwinds}
}

@misc{riederer_rmarkdown_2019,
	title = {{RMarkdown} {Driven} {Development} ({RmdDD})},
	url = {https://emilyriederer.netlify.com/post/rmarkdown-driven-development/},
	abstract = {Introduction RMarkdown is an excellent platform for capturing narrative analysis and code to create reproducible reports, blogs, slides, books, and more. One benefit of RMarkdown is its abilities to keep an analyst in the “flow” of their work and to capture their thought process along the way. However, thought processes are rarely linear; as a result, first-draft RMarkdown scripts rarely are either. This is fine for some individual analysis and preliminary exploration but can significantly decrease how understandable and resilient an RMarkdown will be in the future.},
	language = {en},
	urldate = {2020-01-30},
	journal = {Emily Riederer},
	author = {Riederer, Emily},
	month = may,
	year = {2019}
}

@misc{noauthor_rstudio_nodate,
	title = {{RStudio} {Cloud}},
	url = {https://rstudio.cloud/learn/primers},
	urldate = {2020-01-30}
}

@misc{noauthor_rstudio_nodate-1,
	title = {{RStudio}},
	url = {https://login.rstudio.cloud/login?redirect=%2Foauth%2Fauthorize%3Fredirect_uri%3Dhttps%253A%252F%252Frstudio.cloud%252Flogin%26client_id%3Drstudio-cloud%26response_type%3Dcode%26show_auth%3D0%26show_login%3D0%26show_setup%3D1&setup=True},
	urldate = {2020-01-30}
}

@book{chang_fundamentals_2009,
	title = {Fundamentals of piano practice},
	isbn = {978-1-4196-7859-2},
	abstract = {Most books list what skills are needed (scales, arpeggios, trills, etc.), but not how to acquire them. This book teaches how to solve technical problems, step by step. Learn practice methods, how to acquire technique and memorize hours of repertoire, sight reading, musical playing, relaxation, etc., and, most importantly, mental play in which you learn to play the piano in your mind. Mental play touches every aspect of piano playing, from memorizing, controlling nervousness, developing performance skills, playing musically, etc., to acquiring absolute pitch, composing and improvisation. This book also has a chapter explaining the chromatic scale and temperaments, with detailed instructions on how to tune your own piano.--Publisher's description.},
	language = {en},
	author = {Chang, Chuan C},
	year = {2009},
	note = {OCLC: 878421319}
}

@book{chang_fundamentals_2009-1,
	title = {Fundamentals of piano practice},
	isbn = {978-1-4196-7859-2},
	abstract = {Most books list what skills are needed (scales, arpeggios, trills, etc.), but not how to acquire them. This book teaches how to solve technical problems, step by step. Learn practice methods, how to acquire technique and memorize hours of repertoire, sight reading, musical playing, relaxation, etc., and, most importantly, mental play in which you learn to play the piano in your mind. Mental play touches every aspect of piano playing, from memorizing, controlling nervousness, developing performance skills, playing musically, etc., to acquiring absolute pitch, composing and improvisation. This book also has a chapter explaining the chromatic scale and temperaments, with detailed instructions on how to tune your own piano.--Publisher's description.},
	language = {en},
	author = {Chang, Chuan C},
	year = {2009},
	note = {OCLC: 878421319}
}

@inproceedings{yuan_simple_2014,
	address = {Broomfield, CO},
	series = {{OSDI}'14},
	title = {Simple testing can prevent most critical failures: an analysis of production failures in distributed data-intensive systems},
	isbn = {978-1-931971-16-4},
	shorttitle = {Simple testing can prevent most critical failures},
	abstract = {Large, production quality distributed systems still fail periodically, and do so sometimes catastrophically, where most or all users experience an outage or data loss. We present the result of a comprehensive study investigating 198 randomly selected, user-reported failures that occurred on Cassandra, HBase, Hadoop Distributed File System (HDFS), Hadoop MapReduce, and Redis, with the goal of understanding how one or multiple faults eventually evolve into a user-visible failure. We found that from a testing point of view, almost all failures require only 3 or fewer nodes to reproduce, which is good news considering that these services typically run on a very large number of nodes. However, multiple inputs are needed to trigger the failures with the order between them being important. Finally, we found the error logs of these systems typically contain sufficient data on both the errors and the input events that triggered the failure, enabling the diagnose and the reproduction of the production failures. We found the majority of catastrophic failures could easily have been prevented by performing simple testing on error handling code - the last line of defense - even without an understanding of the software design. We extracted three simple rules from the bugs that have lead to some of the catastrophic failures, and developed a static checker, Aspirator, capable of locating these bugs. Over 30\% of the catastrophic failures would have been prevented had Aspirator been used and the identified bugs fixed. Running Aspirator on the code of 9 distributed systems located 143 bugs and bad practices that have been fixed or confirmed by the developers.},
	urldate = {2020-01-25},
	booktitle = {Proceedings of the 11th {USENIX} conference on {Operating} {Systems} {Design} and {Implementation}},
	publisher = {USENIX Association},
	author = {Yuan, Ding and Luo, Yu and Zhuang, Xin and Rodrigues, Guilherme Renna and Zhao, Xu and Zhang, Yongle and Jain, Pranay U. and Stumm, Michael},
	month = oct,
	year = {2014},
	pages = {249--265}
}

@misc{chang_fundamentals_2009-2,
	title = {Fundamentals of {Piano} {Practice} — {Fundamentals} of {Piano} {Practice}},
	url = {https://fundamentals-of-piano-practice.readthedocs.io/en/latest/},
	abstract = {Chang},
	urldate = {2020-01-25},
	author = {Chang, Chuang},
	year = {2009}
}

@article{blokpoel_deep_2018,
	title = {Deep {Analogical} {Inference} as the {Origin} of {Hypotheses}},
	volume = {11},
	language = {en},
	author = {Blokpoel, Mark and Wareham, Todd and Haselager, Pim and Toni, Ivan and van Rooij, Iris},
	year = {2018},
	pages = {24}
}

@article{devezer_scientific_2019,
	title = {Scientific discovery in a model-centric framework: {Reproducibility}, innovation, and epistemic diversity},
	volume = {14},
	issn = {1932-6203},
	shorttitle = {Scientific discovery in a model-centric framework},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0216125},
	doi = {10.1371/journal.pone.0216125},
	abstract = {Consistent confirmations obtained independently of each other lend credibility to a scientific result. We refer to results satisfying this consistency as reproducible and assume that reproducibility is a desirable property of scientific discovery. Yet seemingly science also progresses despite irreproducible results, indicating that the relationship between reproducibility and other desirable properties of scientific discovery is not well understood. These properties include early discovery of truth, persistence on truth once it is discovered, and time spent on truth in a long-term scientific inquiry. We build a mathematical model of scientific discovery that presents a viable framework to study its desirable properties including reproducibility. In this framework, we assume that scientists adopt a model-centric approach to discover the true model generating data in a stochastic process of scientific discovery. We analyze the properties of this process using Markov chain theory, Monte Carlo methods, and agent-based modeling. We show that the scientific process may not converge to truth even if scientific results are reproducible and that irreproducible results do not necessarily imply untrue results. The proportion of different research strategies represented in the scientific population, scientists’ choice of methodology, the complexity of truth, and the strength of signal contribute to this counter-intuitive finding. Important insights include that innovative research speeds up the discovery of scientific truth by facilitating the exploration of model space and epistemic diversity optimizes across desirable properties of scientific discovery.},
	language = {en},
	number = {5},
	urldate = {2020-01-19},
	journal = {PLOS ONE},
	author = {Devezer, Berna and Nardin, Luis G. and Baumgaertner, Bert and Buzbas, Erkan Ozge},
	month = may,
	year = {2019},
	keywords = {Markov models, Replication studies, Reproducibility, Scientists, Species diversity, Statistical data, Statistical theories, Stochastic processes},
	pages = {e0216125}
}

@misc{noauthor_measuring_nodate,
	title = {Measuring the {Prevalence} of {Questionable} {Research} {Practices} {With} {Incentives} for {Truth} {Telling} - {Leslie} {K}. {John}, {George} {Loewenstein}, {Drazen} {Prelec}, 2012},
	url = {https://journals.sagepub.com/doi/abs/10.1177/0956797611430953?journalCode=pssa},
	urldate = {2020-01-17}
}

@book{gray_panda:_2020,
	title = {panda: {Reproducibility} panda},
	author = {Gray, Charles},
	year = {2020}
}

@book{pedersen_ggforce:_2019,
	title = {ggforce: {Accelerating} 'ggplot2'},
	url = {https://CRAN.R-project.org/package=ggforce},
	author = {Pedersen, Thomas Lin},
	year = {2019}
}

@inproceedings{gray_code::proof:_2019,
	address = {Singapore},
	series = {Communications in {Computer} and {Information} {Science}},
	title = {code::proof: {Prepare} for {Most} {Weather} {Conditions}},
	isbn = {9789811519604},
	shorttitle = {code},
	doi = {10.1007/978-981-15-1960-4_2},
	abstract = {Computational tools for data analysis are being released daily on repositories such as the Comprehensive R Archive Network. How we integrate these tools to solve a problem in research is increasingly complex and requiring frequent updates. To mitigate these Kafkaesque computational challenges in research, this manuscript proposes toolchain walkthrough, an opinionated documentation of a scientific workflow. As a practical complement to our proof-based argument (Gray and Marwick, arXiv, 2019) for reproducible data analysis, here we focus on the practicality of setting up reproducible research compendia, with unit tests, as a measure of code::proof, confidence in computational algorithms.},
	language = {en},
	booktitle = {Statistics and {Data} {Science}},
	publisher = {Springer},
	author = {Gray, Charles T.},
	editor = {Nguyen, Hien},
	year = {2019},
	keywords = {Metaprogramming, Metaresearch, Statistical computing},
	pages = {22--41}
}

@article{box_science_1976,
	title = {Science and {Statistics}},
	volume = {71},
	issn = {0162-1459, 1537-274X},
	url = {http://www.tandfonline.com/doi/abs/10.1080/01621459.1976.10480949},
	doi = {10.1080/01621459.1976.10480949},
	language = {en},
	number = {356},
	urldate = {2020-01-15},
	journal = {Journal of the American Statistical Association},
	author = {Box, George E. P.},
	month = dec,
	year = {1976},
	pages = {791--799}
}

@article{gelman_garden_nodate,
	title = {The garden of forking paths: {Why} multiple comparisons can be a problem, even when there is no “ﬁshing expedition” or “p-hacking” and the research hypothesis was posited ahead of time},
	abstract = {Researcher degrees of freedom can lead to a multiple comparisons problem, even in settings where researchers perform only a single analysis on their data. The problem is there can be a large number of potential comparisons when the details of data analysis are highly contingent on data, without the researcher having to perform any conscious procedure of ﬁshing or examining multiple p-values. We discuss in the context of several examples of published papers where data-analysis decisions were theoretically-motivated based on previous literature, but where the details of data selection and analysis were not pre-speciﬁed and, as a result, were contingent on data.},
	language = {en},
	author = {Gelman, Andrew and Loken, Eric},
	pages = {17}
}

@unpublished{makel_questionable_2019,
	type = {preprint},
	title = {Questionable and {Open} {Research} {Practices} in {Education} {Research}},
	url = {https://osf.io/f7srb},
	abstract = {Discussions of how to improve research quality are predominant in a number of fields, including education. But how prevalent are the use of problematic practices and the improved practices meant to counter them? This baseline information will be a critical data source as education researchers seek to improve our research practices. In this preregistered study, we replicated and extended previous studies from other fields by asking education researchers about 10 questionable research practices and 5 open research practices. We asked them to estimate the prevalence of the practices in the field, self-report their own use of such practices, and estimate the appropriateness of these behaviors in education research. We made predictions under four umbrella categories: comparison to psychology, geographic location, career stage, and quantitative orientation. Broadly, our results suggest that both questionable and open research practices are part of the typical research practices of many educational researchers. Preregistration, code, and data can be found at https://osf.io/83mwk/.},
	urldate = {2020-01-08},
	author = {Makel, Matthew C. and Hodges, Jaret and Cook, Bryan G. and Plucker, Jonathan},
	month = oct,
	year = {2019},
	doi = {10.35542/osf.io/f7srb}
}

@misc{navarro_paths_nodate,
	title = {Paths in strange spaces, part {I}},
	url = {https://djnavarro.net/post/paths-in-strange-spaces/},
	abstract = {The first half of a two-part post on preregistration. In this first half I argue against the proposition that preregistration is a good tool to prevent ‘p-hacking’. In the second half, I will switch gears and talk about why I am more enthusiastic about preregistration as a method to promote transparency.},
	language = {en-us},
	urldate = {2020-01-07},
	author = {Navarro, Danielle}
}

@article{collaboration_estimating_2015,
	title = {Estimating the reproducibility of psychological science},
	volume = {349},
	copyright = {Copyright © 2015, American Association for the Advancement of Science},
	issn = {0036-8075, 1095-9203},
	url = {https://science.sciencemag.org/content/349/6251/aac4716},
	doi = {10.1126/science.aac4716},
	abstract = {Empirically analyzing empirical evidence
One of the central goals in any scientific endeavor is to understand causality. Experiments that seek to demonstrate a cause/effect relation most often manipulate the postulated causal factor. Aarts et al. describe the replication of 100 experiments reported in papers published in 2008 in three high-ranking psychology journals. Assessing whether the replication and the original experiment yielded the same result according to several criteria, they find that about one-third to one-half of the original findings were also observed in the replication study.
Science, this issue 10.1126/science.aac4716
Structured Abstract
INTRODUCTIONReproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. Scientific claims should not gain credence because of the status or authority of their originator but by the replicability of their supporting evidence. Even research of exemplary quality may have irreproducible empirical findings because of random or systematic error.
RATIONALEThere is concern about the rate and predictors of reproducibility, but limited evidence. Potentially problematic practices include selective reporting, selective analysis, and insufficient specification of the conditions necessary or sufficient to obtain the results. Direct replication is the attempt to recreate the conditions believed sufficient for obtaining a previously observed finding and is the means of establishing reproducibility of a finding with new data. We conducted a large-scale, collaborative effort to obtain an initial estimate of the reproducibility of psychological science.
RESULTSWe conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. There is no single standard for evaluating replication success. Here, we evaluated reproducibility using significance and P values, effect sizes, subjective assessments of replication teams, and meta-analysis of effect sizes. The mean effect size (r) of the replication effects (Mr = 0.197, SD = 0.257) was half the magnitude of the mean effect size of the original effects (Mr = 0.403, SD = 0.188), representing a substantial decline. Ninety-seven percent of original studies had significant results (P {\textless} .05). Thirty-six percent of replications had significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.
CONCLUSIONNo single indicator sufficiently describes replication success, and the five indicators examined here are not the only ways to evaluate reproducibility. Nonetheless, collectively these results offer a clear conclusion: A large portion of replications produced weaker evidence for the original findings despite using materials provided by the original authors, review in advance for methodological fidelity, and high statistical power to detect the original effect sizes. Moreover, correlational evidence is consistent with the conclusion that variation in the strength of initial evidence (such as original P value) was more predictive of replication success than variation in the characteristics of the teams conducting the research (such as experience and expertise). The latter factors certainly can influence replication success, but they did not appear to do so here.Reproducibility is not well understood because the incentives for individual scientists prioritize novelty over replication. Innovation is the engine of discovery and is vital for a productive, effective scientific enterprise. However, innovative ideas become old news fast. Journal reviewers and editors may dismiss a new test of a published idea as unoriginal. The claim that “we already know this” belies the uncertainty of scientific evidence. Innovation points out paths that are possible; replication points out paths that are likely; progress relies on both. Replication can increase certainty when findings are reproduced and promote innovation when they are not. This project provides accumulating evidence for many findings in psychological research and suggests that there is still more work to do to verify whether we know what we think we know. {\textless}img class="fragment-image" aria-describedby="F1-caption" src="https://science.sciencemag.org/content/sci/349/6251/aac4716/F1.medium.gif"/{\textgreater} Download high-res image Open in new tab Download Powerpoint Original study effect size versus replication effect size (correlation coefficients).Diagonal line represents replication effect size equal to original effect size. Dotted line represents replication effect size of 0. Points below the dotted line were effects in the opposite direction of the original. Density plots are separated by significant (blue) and nonsignificant (red) effects.
Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. Replication effects were half the magnitude of original effects, representing a substantial decline. Ninety-seven percent of original studies had statistically significant results. Thirty-six percent of replications had statistically significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.
A large-scale assessment suggests that experimental reproducibility in psychology leaves a lot to be desired.
A large-scale assessment suggests that experimental reproducibility in psychology leaves a lot to be desired.},
	language = {en},
	number = {6251},
	urldate = {2020-01-03},
	journal = {Science},
	author = {Collaboration, Open Science},
	month = aug,
	year = {2015},
	pmid = {26315443}
}

@book{gilbert1980madwoman,
	title = {The madwoman in the attic: {The} woman writer and the nineteenth-century literary imagination},
	publisher = {Yale University Press},
	author = {Gilbert, Sandra M and Gubar, Susan},
	year = {1980}
}

@book{bronte2000jane,
	title = {jane eyre},
	publisher = {OUP Oxford},
	author = {Brontë, Charlotte},
	year = {2000}
}

@book{rhys1992wide,
	title = {Wide sargasso sea},
	publisher = {WW Norton \& Company},
	author = {Rhys, Jean},
	year = {1992}
}

@book{kulinskaya2008meta,
	title = {Meta analysis: a guide to calibrating and combining statistical evidence},
	volume = {756},
	publisher = {John Wiley \& Sons},
	author = {Kulinskaya, E. and Morgenthaler, S. and Staudte, R. G.},
	year = {2008}
}

@book{borenstein2008introduction,
	title = {Introduction to metaanalysis},
	publisher = {JSTOR},
	author = {Borenstein, Michael},
	year = {2008}
}

@misc{gray_simeta:_2020,
	title = {simeta: {Simulate} meta-analysis data},
	author = {Gray, Charles},
	year = {2020}
}

@misc{gray_varameta:_2020,
	title = {varameta: {Estimators} for the {Variance} of the {Sample} {Median}},
	author = {Gray, Charles},
	year = {2020}
}

@misc{review_manager_revman_nodate,
	title = {{RevMan} {Web}},
	url = {https://community.cochrane.org/help/tools-and-software/revman-web},
	abstract = {Join our RevMan Web pioneers!
			If you are already a Cochrane review author, then sign up here},
	urldate = {2020-01-02},
	author = {Review Manager}
}

@misc{review_manager_revman_nodate-1,
	title = {{RevMan} 5},
	url = {https://community.cochrane.org/help/tools-and-software/revman-5},
	abstract = {Review Manager 5 (RevMan 5) is the software used for preparing and maintaining Cochrane Reviews.

RevMan facilitates preparation of protocols and full reviews, including text, characteristics of studies, comparison tables, and study data. It can perform meta-analysis of the data entered, and present the results graphically.

You can also use RevMan to write reviews of diagnostic test accuracy studies, reviews of studies of methodology and overviews of reviews.},
	urldate = {2020-01-02},
	author = {Review Manager}
}

@article{shi2018estimate,
	title = {How to estimate the sample mean and standard deviation from the five number summary?},
	journal = {arXiv preprint arXiv:1801.01267},
	author = {Shi, Jiandong and Luo, Dehui and Weng, Hong and Zeng, Xian-Tao and Lin, Lu and Tong, Tiejun},
	year = {2018}
}

@article{RegisteredReports,
	title = {Registered {Reports}},
	url = {https://cos.io/rr/}
}

@misc{noauthor_d-dimer_nodate,
	title = {D-dimer in preeclampsia: {Systematic} review and meta-analysis {\textbar} {Elsevier} {Enhanced} {Reader}},
	shorttitle = {D-dimer in preeclampsia},
	url = {https://reader.elsevier.com/reader/sd/pii/S0009898112004007?token=7298471EFDC11E451E859DFA9CCD81EFD4C1257EE0DD36E97F9E64C4D6EA1B4F9FFF10776D0CC0FF399D84062EAFEB18},
	language = {en},
	urldate = {2020-01-01},
	doi = {10.1016/j.cca.2012.08.003}
}

@article{pinheiro_d-dimer_2012,
	title = {D-dimer in preeclampsia: {Systematic} review and meta-analysis},
	volume = {414},
	issn = {0009-8981},
	shorttitle = {D-dimer in preeclampsia},
	url = {http://www.sciencedirect.com/science/article/pii/S0009898112004007},
	doi = {10.1016/j.cca.2012.08.003},
	abstract = {Preeclampsia is a multifactorial disease characterized by high blood pressure and proteinuria after the 20th week of pregnancy. Preeclampsia is associated with microvasculature fibrin deposition and maternal organ dysfunction. D-dimer (D-Di) has been used as a marker of production/degradation of fibrin in vivo. D-Di has emerged as a useful diagnostic tool for thrombotic conditions because its plasma concentration has a high negative predictive value for venous thromboembolism. The aim of this study was to evaluate publications that assessed plasma D-Di in preeclampsia and normotensive pregnant subjects to define its diagnostic value. A total of 194 publications were identified. Following the exclusion process, seven studies were in accordance with the pre-defined eligibility criteria. This systematic review was performed with methodologic accuracy, including a careful definition of preeclampsia and a high sensitivity literature search strategy. Quality of the included studies was assessed in accordance with widely accepted literature recommendations. Our meta-analysis indicates that increased plasma D-Di is associated with preeclampsia in the third trimester of gestation vs normotensive pregnant subjects. These preliminary findings in this select group of patients clearly highlight the need for additional comprehensive studies throughout pregnancy, including the establishment of an appropriate cut-off, in order to fully elucidate the diagnostic/prognostic role of D-Di in preeclampsia.},
	language = {en},
	urldate = {2020-01-01},
	journal = {Clinica Chimica Acta},
	author = {Pinheiro, Melina de Barros and Junqueira, Daniela Rezende Garcia and Coelho, Fernanda Fonseca and Freitas, Letícia G. and Carvalho, Maria G. and Gomes, Karina Braga and Dusse, Luci Maria Santana},
	month = dec,
	year = {2012},
	keywords = {D-dimer, Diagnosis, Meta-analysis, Preeclampsia, Systematic review},
	pages = {166--170}
}

@misc{noauthor_registered_nodate,
	title = {Registered {Reports}},
	url = {https://cos.io/rr/},
	urldate = {2019-12-30}
}

@article{nosek_preregistration_2019,
	title = {Preregistration {Is} {Hard}, {And} {Worthwhile}},
	volume = {23},
	issn = {1364-6613},
	url = {http://www.sciencedirect.com/science/article/pii/S1364661319301846},
	doi = {10.1016/j.tics.2019.07.009},
	abstract = {Preregistration clarifies the distinction between planned and unplanned research by reducing unnoticed flexibility. This improves credibility of findings and calibration of uncertainty. However, making decisions before conducting analyses requires practice. During report writing, respecting both what was planned and what actually happened requires good judgment and humility in making claims.},
	language = {en},
	number = {10},
	urldate = {2019-12-28},
	journal = {Trends in Cognitive Sciences},
	author = {Nosek, Brian A. and Beck, Emorie D. and Campbell, Lorne and Flake, Jessica K. and Hardwicke, Tom E. and Mellor, David T. and van ’t Veer, Anna E. and Vazire, Simine},
	month = oct,
	year = {2019},
	keywords = {confirmatory research, exploratory research, preregistration, reproducibility, transparency},
	pages = {815--818}
}

@misc{noauthor_making_nodate,
	title = {Making {Black} {Women} {Scientists} under {White} {Empiricism}: {The} {Racialization} of {Epistemology} in {Physics} {\textbar} {Signs}: {Journal} of {Women} in {Culture} and {Society}: {Vol} 45, {No} 2},
	url = {https://www.journals.uchicago.edu/doi/abs/10.1086/704991},
	urldate = {2019-12-08}
}

@article{mccullagh_what_2002,
	title = {What is a statistical model?},
	volume = {30},
	url = {http://projecteuclid.org/euclid.aos/1035844977},
	doi = {10.1214/aos/1035844977},
	language = {en},
	number = {5},
	urldate = {2019-11-21},
	journal = {The Annals of Statistics},
	author = {McCullagh, Peter},
	month = oct,
	year = {2002},
	pages = {1225--1310}
}

@book{antonio_limits_2017,
	title = {Limits {Of} {Mathematical} {Modeling} {In} {The} {Social} {Sciences}, {The}: {The} {Significance} {Of} {Godel}'s {Incompleteness} {Phenomenon}},
	isbn = {978-1-78634-317-8},
	shorttitle = {Limits {Of} {Mathematical} {Modeling} {In} {The} {Social} {Sciences}, {The}},
	abstract = {Current mathematical models are notoriously unreliable in describing the time evolution of unexpected social phenomena, from financial crashes to revolution. Can such events be forecast? Can we compute probabilities about them? Can we model them? This book investigates and attempts to answer these questions through Gödel\&\#39;s two incompleteness theorems, and in doing so demonstrates how influential Gödel is in modern logical and mathematical thinking. Many mathematical models are applied to economics and social theory, while Gödel\&\#39;s theorems are able to predict their limitations for more accurate analysis and understanding of national and international events.This unique discussion is written for graduate level mathematicians applying their research to the social sciences, including economics, social studies and philosophy, and also for formal logicians and philosophers of science.},
	language = {en},
	publisher = {World Scientific},
	author = {Antonio, Doria Francisco},
	month = may,
	year = {2017},
	note = {Google-Books-ID: WvAnDwAAQBAJ},
	keywords = {Mathematics / Applied, Social Science / Methodology, Social Science / Reference}
}

@article{spivak_18-s996:_nodate,
	title = {18-{S}996: {Category} {Theory} for {Scientists}},
	language = {en},
	author = {Spivak, David I},
	pages = {214}
}

@misc{noauthor_category_nodate,
	title = {Category theory for scientists},
	url = {http://math.mit.edu/~dspivak/teaching/sp13/},
	urldate = {2019-11-21}
}

@article{gray2019textttcodeproof,
	title = {code::proof: {Prepare} for most weather conditions},
	author = {Gray, Charles T.},
	year = {2019},
	note = {arXiv: 1910.06964 [stat.OT]}
}

@book{hell_graphs_2004,
	address = {Oxford, New York},
	series = {Oxford {Lecture} {Series} in {Mathematics} and {Its} {Applications}},
	title = {Graphs and {Homomorphisms}},
	isbn = {978-0-19-852817-3},
	abstract = {This is a book about graph homomorphisms. Graph theory is now an established discipline but the study of graph homomorphisms has only recently begun to gain wide acceptance and interest. The subject gives a useful perspective in areas such as graph reconstruction, products, fractional and circular colourings, and has applications in complexity theory, artificial intelligence, telecommunication, and, most recently, statistical physics.Based on the authors' lecture notes for graduate courses, this book can be used as a textbook for a second course in graph theory at 4th year or master's level and has been used for courses at Simon Fraser University (Vancouver), Charles University (Prague), ETH (Zurich), and UFRJ (Rio de Janeiro). The exercises vary in difficulty. The first few are usually intended to give the reader an opportunity to practice the concepts introduced in the chapter; the later ones explore related concepts, or even introduce new ones. For the harder exercises hints and references are provided.The authors are well known for their research in this area and the book will be invaluable to graduate students and researchers alike.},
	publisher = {Oxford University Press},
	author = {Hell, Pavol and Nesetril, Jaroslav},
	month = jul,
	year = {2004}
}

@misc{hatton_full_2016,
	title = {Full {Computational} {Reproducibility} in {Biological} {Science}: {Methods}, {Software} and a {Case} {Study} in {Protein} {Biology}},
	url = {https://arxiv.org/abs/1608.06897},
	urldate = {2019-11-18},
	author = {Hatton, Les and Warr, Gregory},
	year = {2016}
}

@misc{noauthor_paths_nodate,
	title = {Paths in strange spaces},
	url = {https://djnavarro.net/post/paths-in-strange-spaces/},
	abstract = {[work in progress]},
	language = {en-us},
	urldate = {2019-11-15}
}

@misc{noauthor_science_nodate,
	title = {Science and statistics (version 2)},
	url = {https://slides.com/djnavarro/scienceandstatistics2},
	abstract = {Slides for my invited talk for the mathematical psychology satellite day at the 2019 psychonomics conference. This is a variation on an earlier talk I gave at the Aarhus open science workshop.  http://mathpsych.org/conferences/psychonomics2019/},
	urldate = {2019-11-15},
	journal = {Slides}
}

@article{hedges_statistics_2019,
	title = {The {Statistics} of {Replication}},
	volume = {15},
	issn = {1614-1881},
	url = {https://econtent.hogrefe.com/doi/full/10.1027/1614-2241/a000173},
	doi = {10.1027/1614-2241/a000173},
	abstract = {. The concept of replication is fundamental to the logic 					and rhetoric of science, including the argument that science is self-correcting. 					Yet there is very little literature on the methodology of replication. In this 					article, I argue that the definition of replication should not require 					underlying effects to be identical, but should permit some variation in true 					effects to be allowed. I note that different possible analyses could be used to 					determine whether studies replicate. Finally, I argue that a single replication 					study is almost never adequate to determine whether a result replicates. Thus, 					methodological work on the design of replication studies would be useful.},
	number = {Supplement 1},
	urldate = {2019-11-15},
	journal = {Methodology},
	author = {Hedges, Larry V.},
	month = oct,
	year = {2019},
	pages = {3--14}
}

@article{kwisthout_bayesian_2011,
	title = {Bayesian {Intractability} {Is} {Not} an {Ailment} {That} {Approximation} {Can} {Cure}},
	volume = {35},
	issn = {0364-0213},
	url = {https://onlinelibrary.wiley.com/doi/full/10.1111/j.1551-6709.2011.01182.x},
	doi = {10.1111/j.1551-6709.2011.01182.x},
	number = {5},
	urldate = {2019-11-15},
	journal = {Cognitive Science},
	author = {Kwisthout, Johan and Wareham, Todd and van Rooij, Iris},
	month = jul,
	year = {2011},
	pages = {779--784}
}

@misc{simpson_what_2019,
	title = {What if it’s never decorative gourd season? « {Statistical} {Modeling}, {Causal} {Inference}, and {Social} {Science}},
	url = {https://statmodeling.stat.columbia.edu/2019/11/13/what-if-its-never-decorative-gourd-season/},
	urldate = {2019-11-14},
	author = {Simpson, Dan P.},
	year = {2019}
}

@techreport{navarro_between_2018,
	type = {preprint},
	title = {Between the devil and the deep blue sea: {Tensions} between scientific judgement and statistical model selection},
	shorttitle = {Between the devil and the deep blue sea},
	url = {https://osf.io/39q8y},
	abstract = {Discussions of model selection in the psychological literature typically frame the issues as a question of statistical inference, with the goal being to determine which model makes the best predictions about data. Within this setting, advocates of leave-one-out cross-validation and Bayes factors disagree on precisely which prediction problem model selection questions should aim to answer. In this comment, I discuss some of these issues from a scientific perspective. What goal does model selection serve when all models are known to be systematically wrong? How might "toy problems" tell a misleading story? How does the scientific goal of explanation align with (or differ from) traditional statistical concerns? I do not offer answers to these questions, but hope to highlight the reasons why psychological researchers cannot avoid asking them.},
	urldate = {2019-11-11},
	institution = {PsyArXiv},
	author = {Navarro, Danielle},
	month = oct,
	year = {2018},
	doi = {10.31234/osf.io/39q8y}
}

@misc{wagenmakers_breakdown_2019,
	title = {A {Breakdown} of “{Preregistration} is {Redundant}, at {Best}”},
	url = {https://www.bayesianspectacles.org/a-breakdown-of-preregistration-is-redundant-at-best/},
	abstract = {In this sentence-by-sentence breakdown of the paper “Preregistration is Redundant, at Best”, I argue that preregistration is a pragmatic tool to combat biases that invalidate statistical inference.…},
	language = {en-US},
	urldate = {2019-11-09},
	journal = {Bayesian Spectacles},
	author = {Wagenmakers, Eric-Jan},
	month = nov,
	year = {2019}
}

@techreport{szollosi_arrested_2019,
	type = {preprint},
	title = {Arrested theory development: {The} misguided distinction between exploratory and confirmatory research},
	shorttitle = {Arrested theory development},
	url = {https://osf.io/suzej},
	abstract = {Starting from the view that progress in science consists of the improvement of our theories, in the current paper we ask two questions: what makes a theory good, and how much do the current method-oriented solutions to the replication crisis contribute to the development of good theories? Based on contemporary philosophy of science, we argue that good theories are hard-to-vary: they (1) explain what they are supposed to explain, (2) are consistent with other good theories, and (3) cannot easily be adapted to explain anything. Theories can be improved by identifying problems in them either by argument or by experimental test, and then correcting these problems by changing the theory. Importantly, such changes and the resultant theory should only be assessed based on whether they are hard-to-vary. An assessment of the current state of the behavioral sciences reveals that theory development is arrested by the lack of consideration for how easy it is to change theories to account for unexpected observations. Further, most of the current method-oriented solutions are unlikely to contribute much to the development of good theories, because they do not work towards eliminating this problem. Instead, they reward only temporary inflexibility in theories, and promote the assessment of theory change based on whether the theory was changed before (confirmatory) or after (exploratory) an experimental test, but not whether that change yields a hard-to-vary theory. Finally, we argue that these methodological solutions would become irrelevant if we turned our focus to the explicit aim of developing theories that are hard-to-vary.},
	urldate = {2019-11-09},
	institution = {PsyArXiv},
	author = {Szollosi, Aba and Donkin, Chris},
	month = sep,
	year = {2019},
	doi = {10.31234/osf.io/suzej}
}
@techreport{szollosi_preregistration_2019,
	type = {preprint},
	title = {Preregistration is redundant, at best},
	url = {https://osf.io/x36pz},
	abstract = {The key implication argued by proponents of preregistration is that it improves the diagnosticity of statistical tests [1]. In the strong version of this argument, preregistration does this by solving statistical problems, such as family-wise error rates. In the weak version, it nudges people to think more deeply about their theories, methods, and analyses. We argue against both: the diagnosticity of statistical tests depend entirely on how well statistical models map onto underlying theories, and so improving statistical techniques does little to improve theories when the mapping is weak. There is also little reason to expect that preregistration will spontaneously help researchers to develop better theories (and, hence, better methods and analyses).},
	urldate = {2019-11-09},
	institution = {PsyArXiv},
	author = {Szollosi, Aba and Kellen, David and Navarro, Danielle and Shiffrin, Rich and van Rooij, Iris and Van Zandt, Trisha and Donkin, Chris},
	month = oct,
	year = {2019},
	doi = {10.31234/osf.io/x36pz}
}

@book{statrethinkingbook,
	title = {Statistical rethinking: {A} bayesian course with examples in {R} and stan},
	url = {http://xcelab.net/rm/statistical-rethinking/},
	publisher = {CRC Press},
	author = {McElreath, Richard},
	year = {2016}
}

@article{trove.nla.gov.au/work/16869421,
	title = {The {Oxford} {Companion} to {Australian} {Music}},
	language = {English},
	number = {5},
	journal = {Context: Journal of Music Research},
	author = {Bebbington, Warren},
	year = {1993},
	note = {tex.catalogue-url: https://trove.nla.gov.au/work/16869421
tex.subjects: Musicology; Australia
tex.type: Article; Article/Journal or magazine article
tex.url: },
	pages = {45}
}

@book{trove.nla.gov.au/work/7562687,
	edition = {First Vintage books edition},
	title = {Orientalism},
	isbn = {978-0-8041-5386-7},
	url = {http://www.gbv.de/dms/faz-rez/FD1200404132076965.pdf},
	language = {English},
	publisher = {New York : Vintage Books},
	author = {Said, Edward W},
	year = {1979},
	note = {tex.catalogue-url: https://trove.nla.gov.au/work/7562687
tex.contents: Chap. 1: The scope of Orientalism: I. Knowing the Oriental -- II. Imaginative Geography and its representations: Orientalizing the Oriental -- III. Projects -- IV. Crisis -- Chap. 2: Orientalist structures and restructures: I. Redrawn frontiers, redefines issues, secularized religion -- II. Silvestre de Sacy and Ernest Renan: Rational Anthropology and Philological Laboratory -- III. Oriental residence and scholarship: the requirements of Lexicography and imagination -- IV. Pilgrims and pilgrimages, British and French -- Chap. 3: Orientalism now: I. Latent and manifest Orientalism -- II. Style, expertise, vision: Orientalism's worldliness -- III. Modern Anglo-French Orientalism in fullest flower -- IV. The latest phase
tex.subjects: Social Science; HISTORY -- Asia -- General; East and West; Imperialism; Orientalism; Public opinion, Western; Study skills; Oriëntalisme; Orientalisme; Impérialisme; Kultur; Orientalistik; Asia; Middle East; Orient et Occident; Asie -- Opinion publique étrangère; Proche-Orient -- Opinion publique étrangère; Asie -- Étude et enseignement; Proche-Orient -- Étude et enseignement; Orient; Europa; Asia -- Foreign public opinion, Western; Middle East -- Foreign public opinion, Western; Asia -- Study and teaching; Middle East -- Study and teaching
tex.type: Book
Afterword Copyright ©1994 by Edward W. Said }
}

@book{nla.cat-vn1194414,
	title = {Madam {Butterfly} = {Madama} {Butterfly} / {Giacomo} {Puccini}},
	isbn = {0-7145-4038-2},
	language = {English},
	publisher = {Calder ; Riverrum Press London : New York},
	author = {Puccini, , Giacomo and Giacosa, , Giuseppe and Illica, , Luigi and Elkin, R. H. and Belasco, , David and Long, , John Luther and Opera., English National and Royal Opera House (London, England)},
	year = {1984},
	note = {tex.catalogue-url: https://nla.gov.au/nla.cat-vn1194414
tex.life-dates: 1984 -
tex.subjects: Puccini, Giacomo, 1858-1924. Madama Butterfly.; Operas -- Librettos.
tex.type: Book}
}

@book{nla.cat-vn300872,
	title = {Samson et {Dalila} = {Samson} and {Delilah} : opera en 3 actes / poeme de {Fernard} {Lemaire} ; musique de {Camille} {Saint}-{Saens}},
	language = {french},
	publisher = {Durand Paris},
	author = {Saint-Saens, , Camille and Lemaire, Ferdinand. and Oudin, Eugene.},
	note = {tex.catalogue-url: https://nla.gov.au/nla.cat-vn300872
tex.life-dates: 1900 - 1986
tex.subjects: Operas -- Librettos.
tex.type: Book}
}

@book{nla.cat-vn2720507,
	title = {Turandot / {Giacomo} {Puccini}},
	isbn = {0-7145-4039-0},
	language = {English},
	publisher = {J. Calder ; Riverrun Press London : New York},
	author = {Puccini, , Giacomo and Adami, , Giuseppe and Simoni, R. and Gozzi, , Carlo and Opera., English National and Royal Opera House (London, England)},
	year = {1984},
	note = {tex.catalogue-url: https://nla.gov.au/nla.cat-vn2720507
tex.life-dates: 1984 -
tex.subjects: Puccini, Giacomo, 1858-1924. Turandot.; Operas -- Librettos.
tex.type: Book}
}

@book{carroll_annotated_1999,
	address = {New York},
	edition = {Updated, Subsequent edition},
	title = {The {Annotated} {Alice}: {The} {Definitive} {Edition}},
	isbn = {978-0-393-04847-6},
	shorttitle = {The {Annotated} {Alice}},
	abstract = {The culmination of a lifetime of scholarship, The Annotated Alice is a landmark event in the rich history of Lewis Carroll and cause to celebrate the remarkable career of Martin Gardner.For over half a century, Martin Gardner has established himself as one of the world's leading authorities on Lewis Carroll. His Annotated Alice, first published in 1959, has over half a million copies in print around the world and is beloved by both families and scholars―for it was Gardner who first decoded many of the mathematical riddles and wordplay that lay ingeniously embedded in Carroll's two classic stories, Alice's Adventures in Wonderland and Through the Looking Glass.Forty years after this groundbreaking publication, Norton is proud to publish the Definitive Edition of The Annotated Alice, a work that combines the notes of Gardner's 1959 edition with his 1990 volume, More Annotated Alice, as well as additional discoveries drawn from Gardner's encyclopedic knowledge of the texts. Illustrated with John Tenniel's classic, beloved art―along with many recently discovered Tenniel pencil sketches―The Annotated Alice will be Gardner's most beautiful and enduring tribute to Carroll's masterpieces yet. Color, two-tone, and black-and-white photos and illustrations throughout},
	publisher = {W. W. Norton \& Company},
	author = {Carroll, Lewis},
	editor = {Gardner, Martin},
	month = nov,
	year = {1999},
	note = {tex.editora: undefined
tex.editoratype: collaborator}
}

@misc{hayes_testing_2019,
	title = {testing statistical software - aleatoric},
	url = {https://www.alexpghayes.com/blog/testing-statistical-software/},
	abstract = {Do you want some uncertainty with that?},
	language = {English},
	urldate = {2019-06-08},
	author = {Hayes, Alex},
	month = jul,
	year = {2019},
	note = {tex.type: Blog}
}

@book{wickham_r_2015,
	title = {R {Packages}: {Organize}, {Test}, {Document}, and {Share} {Your} {Code}},
	isbn = {978-1-4919-1056-6},
	url = {https://books.google.com.au/books?id=DqSxBwAAQBAJ},
	publisher = {O'Reilly Media},
	author = {Wickham, H.},
	year = {2015},
	note = {tex.lccn: 2015472811}
}

@inproceedings{bertot2008short,
	title = {A short presentation of {Coq}},
	booktitle = {International {Conference} on {Theorem} {Proving} in {Higher} {Order} {Logics}},
	author = {Bertot, Yves},
	year = {2008},
	note = {tex.organization: Springer},
	pages = {12--16}
}

@article{10.1093/biostatistics/kxq028,
	title = {An invitation to reproducible computational research},
	volume = {11},
	issn = {1465-4644},
	url = {https://doi.org/10.1093/biostatistics/kxq028},
	doi = {10.1093/biostatistics/kxq028},
	number = {3},
	journal = {Biostatistics},
	author = {Donoho, David L.},
	month = jul,
	year = {2010},
	note = {tex.eprint: http://oup.prod.sis.lan/biostatistics/article-pdf/11/3/385/716135/kxq028.pdf},
	pages = {385--388}
}

@article{camerer_evaluating_2016,
	title = {Evaluating replicability of laboratory experiments in economics},
	volume = {351},
	issn = {0036-8075, 1095-9203},
	url = {https://science.sciencemag.org/content/351/6280/1433},
	doi = {10.1126/science.aaf0918},
	abstract = {Another social science looks at itself Experimental economists have joined the reproducibility discussion by replicating selected published experiments from two top-tier journals in economics. Camerer et al. found that two-thirds of the 18 studies examined yielded replicable estimates of effect size and direction. This proportion is somewhat lower than unaffiliated experts were willing to bet in an associated prediction market, but roughly in line with expectations from sample sizes and P values. Science, this issue p. 1433 The replicability of some scientific findings has recently been called into question. To contribute data about replicability in economics, we replicated 18 studies published in the American Economic Review and the Quarterly Journal of Economics between 2011 and 2014. All of these replications followed predefined analysis plans that were made publicly available beforehand, and they all have a statistical power of at least 90\% to detect the original effect size at the 5\% significance level. We found a significant effect in the same direction as in the original study for 11 replications (61\%); on average, the replicated effect size is 66\% of the original. The replicability rate varies between 67\% and 78\% for four additional replicability indicators, including a prediction market measure of peer beliefs. By several metrics, economics experiments do replicate, although not as often as predicted. By several metrics, economics experiments do replicate, although not as often as predicted.},
	language = {English},
	number = {6280},
	urldate = {2019-07-03},
	journal = {Science},
	author = {Camerer, Colin F. and Dreber, Anna and Forsell, Eskil and Ho, Teck-Hua and Huber, Jürgen and Johannesson, Magnus and Kirchler, Michael and Almenberg, Johan and Altmejd, Adam and Chan, Taizan and Heikensten, Emma and Holzmeister, Felix and Imai, Taisuke and Isaksson, Siri and Nave, Gideon and Pfeiffer, Thomas and Razen, Michael and Wu, Hang},
	month = mar,
	year = {2016},
	note = {tex.copyright: Copyright © 2016, American Association for the Advancement of Science
tex.pmid: 26940865},
	pages = {1433--1436}
}

@article{hadley_wickham_tidy_2014,
	title = {Tidy {Data}},
	volume = {59},
	issn = {1548-7660},
	url = {https://www.jstatsoft.org/index.php/jss/article/view/v059i10},
	doi = {10.18637/jss.v059.i10},
	language = {English},
	number = {1},
	urldate = {2018-11-26},
	journal = {Journal of Statistical Software},
	author = {{Hadley Wickham}},
	month = sep,
	year = {2014},
	note = {tex.rights: Copyright (c) 2013 Hadley Wickham},
	pages = {1--23}
}

@article{nowogrodzki_how_2019,
	title = {How to support open-source software and stay sane},
	volume = {571},
	url = {http://www.nature.com/articles/d41586-019-02046-0},
	doi = {10.1038/d41586-019-02046-0},
	abstract = {Releasing lab-built open-source software often involves a mountain of unforeseen work for the developers.},
	language = {English},
	urldate = {2019-07-03},
	journal = {Nature},
	author = {Nowogrodzki, Anna},
	month = jul,
	year = {2019},
	note = {tex.copyright: 2019 Nature},
	pages = {133}
}

@article{fraser_questionable_2018,
	title = {Questionable research practices in ecology and evolution},
	volume = {13},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0200303},
	doi = {10.1371/journal.pone.0200303},
	abstract = {We surveyed 807 researchers (494 ecologists and 313 evolutionary biologists) about their use of Questionable Research Practices (QRPs), including cherry picking statistically significant results, p hacking, and hypothesising after the results are known (HARKing). We also asked them to estimate the proportion of their colleagues that use each of these QRPs. Several of the QRPs were prevalent within the ecology and evolution research community. Across the two groups, we found 64\% of surveyed researchers reported they had at least once failed to report results because they were not statistically significant (cherry picking); 42\% had collected more data after inspecting whether results were statistically significant (a form of p hacking) and 51\% had reported an unexpected finding as though it had been hypothesised from the start (HARKing). Such practices have been directly implicated in the low rates of reproducible results uncovered by recent large scale replication studies in psychology and other disciplines. The rates of QRPs found in this study are comparable with the rates seen in psychology, indicating that the reproducibility problems discovered in psychology are also likely to be present in ecology and evolution.},
	language = {English},
	number = {7},
	urldate = {2019-06-27},
	journal = {PLOS ONE},
	author = {Fraser, Hannah and Parker, Tim and Nakagawa, Shinichi and Barnett, Ashley and Fidler, Fiona},
	month = jul,
	year = {2018},
	note = {tex.shortjournal: PLOS ONE},
	keywords = {Behavioral ecology, Community ecology, Ecology and environmental sciences, Evolutionary biology, Evolutionary ecology, Psychology, Publication ethics, Statistical data},
	pages = {e0200303}
}

@article{davey_homomorphism_2018,
	title = {The {Homomorphism} {Lattice} {Induced} by a {Finite} {Algebra}},
	volume = {35},
	copyright = {All rights reserved},
	issn = {1572-9273},
	url = {https://doi.org/10.1007/s11083-017-9426-3},
	doi = {10.1007/s11083-017-9426-3},
	abstract = {Each finite algebra A induces a lattice L A via the quasi-order → on the finite members of the variety generated by A, where B →C if there exists a homomorphism from B to C. In this paper, we introduce the question: ‘Which lattices arise as the homomorphism lattice L A induced by a finite algebra A?’ Our main result is that each finite distributive lattice arises as L Q , for some quasi-primal algebra Q. We also obtain representations of some other classes of lattices as homomorphism lattices, including all finite partition lattices, all finite subspace lattices and all lattices of the form L ⊕1, where L is an interval in the subgroup lattice of a finite group.},
	language = {English},
	number = {2},
	urldate = {2019-06-18},
	journal = {Order},
	author = {Davey, Brian A. and Gray, Charles T. and Pitkethly, Jane G.},
	month = jul,
	year = {2018},
	note = {tex.rights: All rights reserved
tex.shortjournal: Order},
	keywords = {Covering forest, Distributive lattice, Finitely generated variety, Homomorphism order, Quasi-primal algebra},
	pages = {193--214}
}

@article{berger_testing_1987,
	title = {Testing a {Point} {Null} {Hypothesis}: {The} {Irreconcilability} of {P} {Values} and {Evidence}},
	volume = {82},
	issn = {0162-1459},
	shorttitle = {Testing a {Point} {Null} {Hypothesis}},
	url = {https://amstat.tandfonline.com/doi/abs/10.1080/01621459.1987.10478397},
	doi = {10.1080/01621459.1987.10478397},
	abstract = {The problem of testing a point null hypothesis (or a “small interval” null hypothesis) is considered. Of interest is the relationship between the P value (or observed significance level) and conditional and Bayesian measures of evidence against the null hypothesis. Although one might presume that a small P value indicates the presence of strong evidence against the null, such is not necessarily the case. Expanding on earlier work [especially Edwards, Lindman, and Savage (1963) and Dickey (1977)], it is shown that actual evidence against a null (as measured, say, by posterior probability or comparative likelihood) can differ by an order of magnitude from the P value. For instance, data that yield a P value of .05, when testing a normal mean, result in a posterior probability of the null of at least .30 for any objective prior distribution. (“Objective” here means that equal prior weight is given the two hypotheses and that the prior is symmetric and nonincreasing away from the null; other definitions of “objective” will be seen to yield qualitatively similar results.) The overall conclusion is that P values can be highly misleading measures of the evidence provided by the data against the null hypothesis.},
	number = {397},
	urldate = {2019-06-17},
	journal = {Journal of the American Statistical Association},
	author = {Berger, James O. and Sellke, Thomas},
	month = mar,
	year = {1987},
	note = {tex.shortjournal: Journal of the American Statistical Association},
	pages = {112--122}
}

@article{amrhein_scientists_2019,
	title = {Scientists rise up against statistical significance},
	volume = {567},
	url = {http://www.nature.com/articles/d41586-019-00857-9},
	doi = {10.1038/d41586-019-00857-9},
	abstract = {Valentin Amrhein, Sander Greenland, Blake McShane and more than 800 signatories call for an end to hyped claims and the dismissal of possibly crucial effects.},
	number = {7748},
	urldate = {2019-03-20},
	journal = {Nature},
	author = {Amrhein, Valentin and Greenland, Sander and McShane, Blake},
	month = mar,
	year = {2019},
	note = {tex.rights: 2019 Nature},
	keywords = {p-values},
	pages = {305}
}

@article{buzbas_need_2019,
	title = {Need of {Mathematical} {Formalism} in {Proposals} for {Robust} {Modeling}},
	volume = {2},
	issn = {2522-087X},
	url = {https://doi.org/10.1007/s42113-019-00057-8},
	doi = {10.1007/s42113-019-00057-8},
	abstract = {If proposals for improving research practices in modeling make claims or recommendations about validity of statistical procedures, then they have to be also complemented by a formal argument so that the statistical credibility of their claims can be established.},
	language = {en},
	number = {3},
	urldate = {2019-10-25},
	journal = {Computational Brain \& Behavior},
	author = {Buzbas, Erkan},
	month = dec,
	year = {2019},
	pages = {197--199}
}

@misc{bache_magrittr:_2014,
	title = {Magrittr: {A} forward-pipe operator for {R}},
	url = {https://CRAN.R-project.org/package=magrittr},
	author = {Bache, Stefan Milton and Wickham, Hadley},
	year = {2014},
	note = {R package version 1.5}
}

@misc{noauthor_data_nodate,
	title = {Data {Insights}},
	url = {https://blog.davisvaughan.com/2019/10/16/data-frames-as-vectors-of-rows/},
	urldate = {2019-10-19},
	journal = {Data Insights}
}

@misc{noauthor_simulating_nodate,
	title = {Simulating data with {Bayesian} networks {\textbar} {R}-bloggers},
	url = {https://www.r-bloggers.com/simulating-data-with-bayesian-networks/},
	urldate = {2019-10-16}
}

@misc{mcbain_datapasta:_2018,
	title = {Datapasta: {R} tools for data copy-pasta},
	url = {https://CRAN.R-project.org/package=datapasta},
	author = {McBain, Miles and Carroll, Jonathan},
	year = {2018},
	note = {R package version 3.0.0}
}

@inproceedings{klees_evaluating_2018,
	address = {New York, NY, USA},
	series = {{CCS} '18},
	title = {Evaluating {Fuzz} {Testing}},
	isbn = {978-1-4503-5693-0},
	url = {http://doi.acm.org/10.1145/3243734.3243804},
	doi = {10.1145/3243734.3243804},
	abstract = {Fuzz testing has enjoyed great success at discovering security critical bugs in real software. Recently, researchers have devoted significant effort to devising new fuzzing techniques, strategies, and algorithms. Such new ideas are primarily evaluated experimentally so an important question is: What experimental setup is needed to produce trustworthy results? We surveyed the recent research literature and assessed the experimental evaluations carried out by 32 fuzzing papers. We found problems in every evaluation we considered. We then performed our own extensive experimental evaluation using an existing fuzzer. Our results showed that the general problems we found in existing experimental evaluations can indeed translate to actual wrong or misleading assessments. We conclude with some guidelines that we hope will help improve experimental evaluations of fuzz testing algorithms, making reported results more robust.},
	urldate = {2019-10-14},
	booktitle = {Proceedings of the 2018 {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {ACM},
	author = {Klees, George and Ruef, Andrew and Cooper, Benji and Wei, Shiyi and Hicks, Michael},
	year = {2018},
	note = {event-place: Toronto, Canada},
	keywords = {evaluation, fuzzing, security},
	pages = {2123--2138}
}

@misc{skyisup_deadly_2019,
	title = {Deadly {Boss} {Mods} {Addon} {Guide}},
	url = {https://www.wowhead.com/deadly-boss-mods-addon-guide},
	abstract = {Guide to utilizing Deadly Boss Mods (DBM), a popular addon that assists players on tracking and reacting to dungeon and raid boss mechanics in World of Warcraft.},
	urldate = {2019-10-08},
	journal = {Wowhead},
	author = {skyisup},
	year = {2019}
}

@unpublished{grainger_maximising_2019,
	title = {Maximising the leverage of existing knowledge could reduce research waste in applied ecology and conservation},
	url = {https://osf.io/42fkh},
	abstract = {Where research does not lead to benefits to society, it may be considered a waste of resources, especially when publicly funded. A formal assessment of the accumulated knowledge prior to research approval would reduce the waste of already limited resources caused by asking low priority questions. There is an urgent need for a change in research workflows so that pre-existing knowledge is better utilised in designing new research.},
	urldate = {2019-10-03},
	author = {Grainger, Matthew and Bolam, Friederike C and stewart, Gavin and Nilsen, Erlend B.},
	month = sep,
	year = {2019},
	doi = {10.32942/osf.io/42fkh}
}

@misc{noauthor_contempt_nodate,
	title = {Contempt {Culture} - {The} {Particular} {Finest}},
	url = {https://blog.aurynn.com/2015/12/16-contempt-culture#footnote-1GRF},
	abstract = {Other languages},
	urldate = {2019-10-03}
}

@misc{noauthor_new_nodate,
	title = {A {New} {Map} for {Relationships} {Co}-authors {Martin} \& {Dorothie} {Hellman} ebook},
	url = {https://anewmap.com/},
	abstract = {Co-authors Martin and Dorothie Hellman reveal a new map to creating a healthy marriage and how its principles can create peace on our planet. Download ebook},
	language = {en-US},
	urldate = {2019-09-26},
	journal = {A New Map}
}

@article{carney_new_2018,
	title = {{THE} {NEW} {DIGITAL} {FUTURE} {FOR} {WELFARE}: {DEBTS} {WITHOUT} {LEGAL} {PROOFS} {OR} {MORAL} {AUTHORITY}?},
	language = {en},
	author = {Carney, Terry},
	year = {2018},
	pages = {16}
}

@article{jankowski_perspective_2020,
	title = {Perspective on coarse-graining, cognitive load, and materials simulation},
	volume = {171},
	issn = {0927-0256},
	url = {http://www.sciencedirect.com/science/article/pii/S0927025619304203},
	doi = {10.1016/j.commatsci.2019.109129},
	abstract = {The predictive capabilities of computational materials science today derive from overlapping advances in simulation tools, modeling techniques, and best practices. We outline this ecosystem of molecular simulations by explaining how important contributions in each of these areas have fed into each other. The combined output of these tools, techniques, and practices is the ability for researchers to advance understanding by efficiently combining simple models with powerful software. As specific examples, we show how the prediction of organic photovoltaic morphologies have improved by orders of magnitude over the last decade, and how the processing of reacting epoxy thermosets can now be investigated with million-particle models. We discuss these two materials systems and the training of materials simulators through the lens of cognitive load theory. For students, the broad view of ecosystem components should facilitate understanding how the key parts relate to each other first, followed by targeted exploration. In this way, the paper is organized in loose analogy to a coarse-grained model: The main components provide basic framing and accelerated sampling from which deeper research is better contextualized. For mentors, this paper is organized to provide a snapshot in time of the current simulation ecosystem and an on-ramp for simulation experts into the literature on pedagogical practice.},
	urldate = {2019-09-06},
	journal = {Computational Materials Science},
	author = {Jankowski, Eric and Ellyson, Neale and Fothergill, Jenny W. and Henry, Michael M. and Leibowitz, Mitchell H. and Miller, Evan D. and Alberts, Mone’t and Chesser, Samantha and Guevara, Jaime D. and Jones, Chris D. and Klopfenstein, Mia and Noneman, Kendra K. and Singleton, Rachel and Uriarte-Mendoza, Ramon A. and Thomas, Stephen and Estridge, Carla E. and Jones, Matthew L.},
	month = jan,
	year = {2020},
	keywords = {Coarse-grained models, Cognitive load, Epoxy, Molecular dynamics, Organic photovoltaics, Software, Teaching, Thermodynamics, Thermosets},
	pages = {109129}
}

@article{hopkin_palaeontology_2007,
	title = {Palaeontology journal will 'fuel black market'},
	volume = {445},
	copyright = {2007 Nature Publishing Group},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/445234b},
	doi = {10.1038/445234b},
	abstract = {Academics say published fossil finds should be available for study.},
	language = {en},
	urldate = {2019-09-05},
	journal = {Nature},
	author = {Hopkin, Michael},
	month = jan,
	year = {2007},
	pages = {234--235}
}

@article{Bryan2017ExcuseMD,
	title = {Excuse me, do you have a moment to talk about version control?},
	volume = {5},
	journal = {PeerJ PrePrints},
	author = {Bryan, Jennifer},
	year = {2017},
	pages = {e3159}
}

@misc{aust_citr:_2018,
	title = {Citr: '{RStudio}' add-in to insert markdown citations},
	url = {https://github.com/crsh/citr},
	author = {Aust, Frederik},
	year = {2018},
	note = {R package version 0.3.0}
}

@book{long_r_2019,
	title = {R {Cookbook}, 2nd {Edition}},
	url = {https://rc2e.com/},
	abstract = {Second edition of R Cookbook},
	urldate = {2019-09-02},
	author = {Long, James (JD) and Teetor, Paul},
	year = {2019}
}

@book{grolemund_r_2017,
	title = {R for {Data} {Science}},
	url = {https://r4ds.had.co.nz/},
	abstract = {This book will teach you how to do data science with R: You’ll learn how to get your data into R, get it into the most useful structure, transform it, visualise it and model it. In this book, you will find a practicum of skills for data science. Just as a chemist learns how to clean test tubes and stock a lab, you’ll learn how to clean data and draw plots—and many other things besides. These are the skills that allow data science to happen, and here you will find the best practices for doing each of these things with R. You’ll learn how to use the grammar of graphics, literate programming, and reproducible research to save time. You’ll also learn how to manage cognitive resources to facilitate discoveries when wrangling, visualising, and exploring data.},
	urldate = {2019-09-02},
	author = {Grolemund, Garrett and Wickham, Hadley},
	year = {2017}
}

@misc{noauthor_learning_nodate,
	title = {Learning {Statistics} with {R}},
	url = {https://learningstatisticswithr.com},
	urldate = {2019-09-02}
}

@misc{rstudio_rstudio_nodate,
	title = {{RStudio}},
	url = {https://www.rstudio.com/},
	abstract = {Open source and enterprise-ready professional software for data science.},
	urldate = {2019-09-02},
	author = {RStudio}
}

@article{viechtbauer_conducting_2010,
	title = {Conducting meta-analyses in {R} with the {\textless}span class="nocase"{\textgreater}metafor{\textless}/span{\textgreater} package},
	volume = {36},
	url = {http://www.jstatsoft.org/v36/i03/},
	number = {3},
	journal = {Journal of Statistical Software},
	author = {Viechtbauer, Wolfgang},
	year = {2010},
	pages = {1--48}
}

@misc{wickham_roxygen2:_2019,
	title = {Roxygen2: in-line documentation for {R}},
	url = {https://github.com/klutometis/roxygen},
	author = {Wickham, Hadley and Danenberg, Peter and Eugster, Manuel},
	year = {2019},
	note = {R package version 6.1.1.9000}
}

@book{serfling_approximation_2009,
	title = {Approximation {Theorems} of {Mathematical} {Statistics}},
	isbn = {978-0-470-31719-8},
	abstract = {Approximation Theorems of Mathematical Statistics  This convenient paperback edition makes a seminal text in statistics accessible to a new generation of students and practitioners. Approximation Theorems of Mathematical Statistics covers a broad range of limit theorems useful in mathematical statistics, along with methods of proof and techniques of application. The manipulation of "probability" theorems to obtain "statistical" theorems is emphasized. Besides a knowledge of these basic statistical theorems, this lucid introduction to the subject imparts an appreciation of the instrumental role of probability theory.  The book makes accessible to students and practicing professionals in statistics, general mathematics, operations research, and engineering the essentials of: * The tools and foundations that are basic to asymptotic theory in statistics * The asymptotics of statistics computed from a sample, including transformations of vectors of more basic statistics, with emphasis on asymptotic distribution theory and strong convergence * Important special classes of statistics, such as maximum likelihood estimates and other asymptotic efficient procedures; W. Hoeffding's U-statistics and R. von Mises's "differentiable statistical functions" * Statistics obtained as solutions of equations ("M-estimates"), linear functions of order statistics ("L-statistics"), and rank statistics ("R-statistics") * Use of influence curves * Approaches toward asymptotic relative efficiency of statistical test procedures},
	language = {en},
	publisher = {John Wiley \& Sons},
	author = {Serfling, Robert J.},
	month = sep,
	year = {2009},
	note = {Google-Books-ID: enUouJ4EHzQC},
	keywords = {Mathematics / Probability \& Statistics / General, Mathematics / Probability \& Statistics / Stochastic Processes}
}

@article{wolpert_no_1997,
	title = {No free lunch theorems for optimization},
	volume = {1},
	issn = {1089778X},
	url = {http://ieeexplore.ieee.org/document/585893/},
	doi = {10.1109/4235.585893},
	abstract = {A framework is developed to explore the connection between effective optimization algorithms and the problems they are solving. A number of “no free lunch” (NFL) theorems are presented which establish that for any algorithm, any elevated performance over one class of problems is offset by performance over another class. These theorems result in a geometric interpretation of what it means for an algorithm to be well suited to an optimization problem. Applications of the NFL theorems to information-theoretic aspects of optimization and benchmark measures of performance are also presented. Other issues addressed include time-varying optimization problems and a priori “head-to-head” minimax distinctions between optimization algorithms, distinctions that result despite the NFL theorems’ enforcing of a type of uniformity over all algorithms.},
	language = {en},
	number = {1},
	urldate = {2019-08-30},
	journal = {IEEE Transactions on Evolutionary Computation},
	author = {Wolpert, D.H. and Macready, W.G.},
	month = apr,
	year = {1997},
	pages = {67--82}
}

@article{hozo_estimating_2005,
	title = {Estimating the mean and variance from the median, range, and the size of a sample},
	volume = {5},
	issn = {1471-2288},
	url = {https://doi.org/10.1186/1471-2288-5-13},
	doi = {10.1186/1471-2288-5-13},
	abstract = {Usually the researchers performing meta-analysis of continuous outcomes from clinical trials need their mean value and the variance (or standard deviation) in order to pool data. However, sometimes the published reports of clinical trials only report the median, range and the size of the trial.},
	number = {1},
	urldate = {2019-08-30},
	journal = {BMC Medical Research Methodology},
	author = {Hozo, Stela Pudar and Djulbegovic, Benjamin and Hozo, Iztok},
	month = apr,
	year = {2005},
	pages = {13}
}

@article{bland_estimating_2014,
	title = {Estimating {Mean} and {Standard} {Deviation} from the {Sample} {Size}, {Three} {Quartiles}, {Minimum}, and {Maximum}},
	volume = {4},
	copyright = {Policy for Journals/Articles with Open Access Authors who publish with this journal agree to the following terms:    Authors retain copyright and grant the journal right of first publication with the work simultaneously licensed under a  Creative Commons Attribution License  that allows others to share the work with an acknowledgement of the work's authorship and initial publication in this journal.      Authors are permitted and encouraged to post links to their work online (e.g., in institutional repositories or on their website) prior to and during the submission process, as it can lead to productive exchanges, as well as earlier and greater citation of published work   Policy for Journals / Manuscript with Paid Access Authors who publish with this journal agree to the following terms:    Publisher retain copyright .      Authors are permitted and encouraged to post links to their work online (e.g., in institutional repositories or on their website) prior to and during the submission process, as it can lead to productive exchanges, as well as earlier and greater citation of published work},
	issn = {1929-6029},
	url = {http://lifescienceglobal.com/pms/index.php/ijsmr/article/view/2688},
	abstract = {Background: We sometimes want to include in a meta-analysis data from studies where results are presented as medians and ranges or interquartile ranges rather than as means and standard deviations. In this paper I extend a method of Hozo et al. to estimate mean and standard deviation from median, minimum, and maximum to the case where quartiles are also available.Methods: Inequalities are developed for each observation using upper and lower limits derived from the minimum, the three quartiles, and the maximum. These are summed to give bounds for the sum and hence the mean of the observations, the average of these bounds in the estimate. A similar estimate is found for the sum of the observations squared and hence for the variance and standard deviation.Results: For data from a Normal distribution, the extended method using quartiles gives good estimates of sample means but sample standard deviations are overestimated. For data from a Lognormal distribution, both sample mean and standard deviation are overestimated. Overestimation is worse for larger samples and for highly skewed parent distributions. The extended estimates using quartiles are always superior in both bias and precision to those without.Conclusions: The estimates have the advantage of being extremely simple to carry out. I argue that as, in practice, such methods will be applied to small samples, the overestimation may not be a serious problem.},
	language = {en},
	number = {1},
	urldate = {2019-08-30},
	journal = {International Journal of Statistics in Medical Research},
	author = {Bland, Martin},
	month = jan,
	year = {2014},
	keywords = {Quartile, maximum, mean, minimum, standard deviation, systematic review.},
	pages = {57--64--64}
}

@article{wan_estimating_2014,
	title = {Estimating the sample mean and standard deviation from the sample size, median, range and/or interquartile range},
	volume = {14},
	issn = {1471-2288},
	url = {https://doi.org/10.1186/1471-2288-14-135},
	doi = {10.1186/1471-2288-14-135},
	abstract = {In systematic reviews and meta-analysis, researchers often pool the results of the sample mean and standard deviation from a set of similar clinical trials. A number of the trials, however, reported the study using the median, the minimum and maximum values, and/or the first and third quartiles. Hence, in order to combine results, one may have to estimate the sample mean and standard deviation for such trials.},
	number = {1},
	urldate = {2019-08-30},
	journal = {BMC Medical Research Methodology},
	author = {Wan, Xiang and Wang, Wenqian and Liu, Jiming and Tong, Tiejun},
	month = dec,
	year = {2014},
	pages = {135}
}

@inproceedings{law_statistical_2015,
	title = {Statistical analysis of simulation output data: {The} practical state of the art},
	shorttitle = {Statistical analysis of simulation output data},
	doi = {10.1109/WSC.2015.7408297},
	abstract = {One of the most important but neglected aspects of a simulation study is the proper design and analysis of simulation experiments. In this tutorial we give a state-of-the-art presentation of what the practitioner really needs to know to be successful. We will discuss how to choose the simulation run length, the warmup-period duration (if any), and the required number of model replications (each using different random numbers). The talk concludes with a discussion of three critical pitfalls in simulation output-data analysis.},
	booktitle = {2015 {Winter} {Simulation} {Conference} ({WSC})},
	author = {Law, A. M.},
	month = dec,
	year = {2015},
	keywords = {Subspace constraints, model replications, random numbers, simulation, simulation experiments, simulation output data, simulation output-data analysis, statistical analysis, warmup-period duration},
	pages = {1810--1819}
}

@misc{andersen_automatic_2019,
	title = {Automatic 'testthat' test skeletons with new {R} package 'roxytest' extending 'roxygen2'},
	url = {https://mikl.dk/post/2019-roxytest/},
	abstract = {It is important to test software. One approach is unit-testing, and for R packages this can e.g. be done using testthat.
It is also important to document software. For R packages roxygen2 is really helpful: It enables you to write documentation in the code file in the R/ folder where the function is implemented. And then roxygen2 takes care of handling the Rd files in the man/ folder.
I have made a new R package that combines these approaches: roxytest.},
	language = {en-us},
	urldate = {2019-08-25},
	journal = {Mikkel Meyer Andersen},
	author = {Andersen, Mikkel Meyer},
	month = aug,
	year = {2019}
}

@article{schmidt_tool-driven_2019,
	title = {Tool-driven {Revolutions} in {Archaeological} {Science}},
	url = {https://osf.io/preprints/socarxiv/4nkxv/},
	doi = {10.31235/osf.io/4nkxv},
	abstract = {There is an argument in philosophy of science that revolutions in science are either ideas-driven or tool-driven. We explore this debate in light of recent efforts by many scientific disciplines to embrace methods to improve the reproducibility of their research. One of the most profound changes driven by this concern for reproducibility and transparency is from analysing data using tools dependent on pointing-and-clicking with a mouse in commercial software, to a new set of tools based on writing scripts in free and open source programming languages. We present bibliometric evidence for this change in the sciences generally and in archaeology in particular to test if the adoption of these new tools may be considered a revolution or some other process. We identify a citation effect for papers that use code. We discuss how computational approaches to improving reproducibility and transparency in archaeology are mediated and transformed by the use of code.},
	urldate = {2019-08-24},
	author = {Schmidt, Sophie and Marwick, Ben},
	month = jan,
	year = {2019}
}

@article{laine_open_2018,
	title = {Open science and codes of conduct on research integrity},
	volume = {37},
	copyright = {Copyright (c) 2018},
	issn = {1797-9129},
	url = {https://journal.fi/inf/article/view/77414},
	doi = {10.23978/inf.77414},
	language = {en},
	number = {4},
	urldate = {2019-08-23},
	journal = {Informaatiotutkimus},
	author = {Laine, Heidi},
	month = dec,
	year = {2018}
}

@article{chua_rioting_2007,
	title = {Rioting with {Stravinsky}: a {Particular} {Analysis} of the {Rite} of {Spring}},
	volume = {26},
	copyright = {© 2007 The Author. Journal compilation © 2007 Blackwell Publishing Ltd},
	issn = {1468-2249},
	shorttitle = {Rioting with {Stravinsky}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1468-2249.2007.00250.x},
	doi = {10.1111/j.1468-2249.2007.00250.x},
	abstract = {The riot that greeted the Rite of Spring only found its analytical counterpart some 70 years after its première, with factions headed by Pieter van den Toorn, Richard Taruskin and Allen Forte. But the analytical tussle was hardly a genuine riot in as much the differing camps subscribed to a premise of authenticity in order to stabilise the work under a universal concept: the result was a Rite unified by theory. The scholars may have battled with each other, but the music was not allowed to have its own riot. This article suggests a more contingent analysis of the Rite, focussing on the rebellion of the particular against the universal. The point is not to champion an anarchic or barbaric reading of the music, which is often attributed to the work because of the ballet's violent content, but to open the possibility of a new order that arises from the rioting particular.},
	language = {en},
	number = {1-2},
	urldate = {2019-08-23},
	journal = {Music Analysis},
	author = {Chua, Daniel K. L.},
	year = {2007},
	pages = {59--109}
}

@article{locke_exoticism_2012,
	title = {On {Exoticism}, {Western} {Art} {Music}, and the {Words} {We} {Use}},
	volume = {69},
	issn = {0003-9292},
	url = {https://www.jstor.org/stable/23375158},
	abstract = {Studies of musikalische Terminologie have helped make scholars aware of the changing nature of the words we use in our own writings and in words we encounter in the musical world around us (e.g., in music journalism). The present article studies the terms (or concepts) "exoticism" and "Western classical music" as well as the interrelationship between them. Also explored are sometimes-hidden implications with regard to ethnic identity and "race." Is Tan Dun writing exotic music when he is evoking his own people's history for New York's Metropolitan Opera?},
	number = {4},
	urldate = {2019-08-23},
	journal = {Archiv für Musikwissenschaft},
	author = {Locke, Ralph P.},
	year = {2012},
	pages = {318--328}
}

@misc{bancroft_mulan_1998,
	title = {Mulan},
	url = {http://www.imdb.com/title/tt0120762/},
	abstract = {Directed by Tony Bancroft, Barry Cook.  With Ming-Na Wen, Eddie Murphy, BD Wong, Miguel Ferrer. To save her father from death in the army, a young maiden secretly goes in his place and becomes one of China's greatest heroines in the process.},
	urldate = {2019-08-22},
	author = {Bancroft, Tony and Cook, Barry},
	year = {1998},
	note = {IMDb ID: tt0120762}
}

@misc{noauthor_orientalism_2019,
	title = {\textit{{Orientalism}} (book)},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Orientalism_(book)&oldid=901324702},
	abstract = {Orientalism is a 1978 book by Edward W. Said, in which the author discusses Orientalism, defined as the West's patronizing representations of "The East"—the societies and peoples who inhabit the places of Asia, North Africa, and the Middle East. According to Said, orientalism (the Western scholarship about the Eastern World) is inextricably tied to the imperialist societies who produced it, which makes much Orientalist work inherently political and servile to power.According to Said, in the Middle East, the social, economic, and cultural practices of the ruling Arab elites indicate they are imperial satraps who have internalized the romanticized "Arab Culture" created by French, British and, later, American Orientalists; the examples include critical analyses of the colonial literature of Joseph Conrad, which conflates a people, a time, and a place into a narrative of incident and adventure in an exotic land.The critical application of post-structuralism in the scholarship of Orientalism influenced the development of literary theory, cultural criticism, and the field of Middle Eastern studies, especially regarding how academics practice their intellectual inquiry when examining, describing, and explaining the Middle East. The scope of Said's scholarship established Orientalism as a foundation text in the field of post-colonial culture studies, which examines the denotations and connotations of Orientalism, and the history of a country's post-colonial period.As a public intellectual, Edward Said debated Orientalism with historians and scholars of area studies, notably, the historian Bernard Lewis, who described the thesis of Orientalism as "anti-Western". For subsequent editions of Orientalism, Said wrote an "Afterword" (1995) and a "Preface" (2003) addressing criticisms of the content, substance, and style of the work as cultural criticism.},
	language = {en},
	urldate = {2019-08-22},
	journal = {Wikipedia},
	month = jun,
	year = {2019},
	note = {Page Version ID: 901324702}
}

@book{dostoyevsky_crime_2004,
	title = {Crime and punishment},
	isbn = {978-0-7434-8763-4},
	url = {https://trove.nla.gov.au/work/5021225},
	abstract = {Raskolnikov commits murder. He then must deal both with the police, and his own guilty conscience. Determined to overreach his humanity and assert his untrammelled individual will, Raskolnikov, an impoverished student living in the St. Petersburg of the Tsars, commits an act of murder and theft and sets into motion a story which, for its excrutiating suspense, its atmospheric vividness, and its profundity of characterization and vision, is almost unequaled in the literatures of the world. The best known of Dostoevsky's masterpieces, Crime and Punishment can bear any amount of rereading without losing a drop of its power over our imagination.  Saint Petersburg (Russia) - Fiction.; Murder - Russia (Federation) - Saint Petersburg - Fiction.; Psychological fiction.},
	urldate = {2019-08-22},
	publisher = {New York Pocket Books},
	author = {Dostoyevsky, Fyodor and Brantley, Margaret and Congress), Copyright Paperback Collection},
	year = {2004},
	keywords = {Crime and Punishment}
}

@misc{noauthor_save_nodate,
	title = {Save {Time} and {Improve} your {Marks} with {CiteThisForMe}, {The} {No}. 1 {Citation} {Tool}},
	url = {http://www.citethisforme.com},
	abstract = {Automatically reference everything correctly with CiteThisForMe. Save your work forever, build multiple bibliographies, run plagiarism checks, and much more.},
	language = {en},
	urldate = {2019-08-22},
	journal = {Cite This For Me}
}

@book{wharton_age_1920,
	title = {The {Age} of {Innocence}},
	copyright = {Public domain in the USA.},
	url = {http://www.gutenberg.org/ebooks/541},
	language = {English},
	urldate = {2019-08-22},
	author = {Wharton, Edith},
	year = {1920},
	keywords = {Domestic fiction, Love stories, Married people -- Fiction, New York (N.Y.) -- Fiction, Separated people -- Fiction, Triangles (Interpersonal relations) -- Fiction, Upper class -- Fiction}
}

@misc{noauthor_age_nodate,
	title = {The {Age} of {Innocence}},
	url = {http://www.imdb.com/title/tt0106226/},
	abstract = {Directed by Martin Scorsese.  With Daniel Day-Lewis, Michelle Pfeiffer, Winona Ryder, Linda Faye Farkas. A tale of nineteenth-century New York high society in which a young lawyer falls in love with a woman separated from her husband, while he is engaged to the woman's cousin.},
	urldate = {2019-08-22},
	note = {IMDb ID: tt0106226}
}

@misc{gray_coding_2019,
	title = {Coding of {Kindness}: {Let}’s figure out how to be better humans to each other},
	url = {https://softloud.github.io/codingofkindness/},
	abstract = {Let’s figure out how to be better humans to each other},
	urldate = {2019-08-22},
	journal = {Coding of Kindness},
	author = {Gray, Charles T. and Marwick, Ben and Richmond, Jen and Kothe, Emily and Ling, Mathew and Knaus, Brian},
	year = {2019}
}

@book{winterson_art_1996,
	address = {New York, N.Y.},
	title = {Art \& {Lies}},
	isbn = {978-0-679-76270-6},
	abstract = {One of the most audacious and provocative writers on either side of the Atlantic now gives readers a dazzling, arousing, and wise improvisation on art, Eros, language, and identity. "A series of intense, artful musings that are exhilarating and visionary. . . . Unsettling yet strangely satisfying."--Newsday.},
	language = {English},
	publisher = {Vintage},
	author = {Winterson, Jeanette},
	month = feb,
	year = {1996}
}

@misc{sandler_ring_2019,
	title = {Ring {Theory} {Helps} {Us} {Bring} {Comfort} {In}},
	url = {https://www.psychologytoday.com/blog/promoting-hope-preventing-suicide/201705/ring-theory-helps-us-bring-comfort-in},
	abstract = {And “dump” our own stuff out.},
	language = {en-US},
	urldate = {2019-08-18},
	journal = {Psychology Today},
	author = {Sandler, Elana},
	month = may,
	year = {2019}
}
@misc{mcewan_harmful_2011,
	title = {Harmful {Communication}, {Part} {One}: {Intent} {Is} {Magic}},
	shorttitle = {Harmful {Communication}, {Part} {One}},
	url = {http://www.shakesville.com/2011/12/harmful-communication-part-one-intent.html},
	abstract = {[Trigger warning for harmful language, emotional manipulation, rape culture.]   This is the first post in a series about language.  Specific...},
	urldate = {2019-08-16},
	author = {McEwan, Melissa},
	month = jan,
	year = {2011}
}

@misc{noauthor_guide_nodate,
	title = {Guide to {Allyship}},
	url = {http://www.guidetoallyship.com/},
	urldate = {2019-08-15}
}

@misc{whedon_serenity_2005,
	title = {Serenity},
	url = {http://www.imdb.com/title/tt0379786/},
	abstract = {Directed by Joss Whedon.  With Nathan Fillion, Gina Torres, Chiwetel Ejiofor, Alan Tudyk. The crew of the ship Serenity try to evade an assassin sent to recapture one of their members who is telepathic.},
	urldate = {2019-08-15},
	author = {Whedon, Joss},
	year = {2005},
	note = {IMDb ID: tt0379786}
}

@book{harrison_profiling_2012,
	title = {Profiling for profit : a report on target marketing and profiling practices in the credit industry},
	copyright = {2012, Consumer Action Law Centre},
	isbn = {978-0-9804788-5-3},
	shorttitle = {Profiling for profit},
	url = {http://dro.deakin.edu.au/view/DU:30064922},
	abstract = {This report examines how many businesses make significant investments to purchase and develop customer relationship management systems. Given such investments, information about these systems is not widely available, but some publicly available information gives indication of the extent, and purpose, of the use. Recognising that lenders use customer information and highly sophisticated systems to target their marketing strategies, is the first step towards ensuring that these practices are taken into account in the development of consumer policy and law reform. This research was funded by the consumer advisory panel of the Australian Securities and Investment Commission (ASIC).},
	language = {eng},
	urldate = {2019-08-14},
	publisher = {Consumer Action Law Centre},
	author = {Harrison, Paul and Gray, Charles Ti},
	month = jan,
	year = {2012}
}

@article{harrison_ethical_2010,
	title = {The ethical and policy implications of profiling ‘vulnerable’ customers: {Implications} of customer profiling},
	volume = {34},
	issn = {14706423, 14706431},
	shorttitle = {The ethical and policy implications of profiling ‘vulnerable’ customers},
	url = {http://doi.wiley.com/10.1111/j.1470-6431.2010.00873.x},
	doi = {10.1111/j.1470-6431.2010.00873.x},
	abstract = {In the shadow of the global ﬁnancial crisis, the issue of the marketing of credit has become an increasing concern in the past 12 months. Outstanding personal debt in the UK currently stands at £1479 billion and is rising by £1 million every 10.6 min. In Australia, there is currently \$44.6 billion worth of outstanding credit card debt, and in the US, \$2596 billion was owed on credit cards in 2008. At present, the banking sector utilizes sophisticated research methods to proﬁle consumers, including those who might be considered ﬁnancially vulnerable. However, the policy frameworks in most industrialized countries do not account for this form of target marketing when considering how to protect vulnerable groups. This paper is an initial attempt to examine the different methods by which proﬁling is conducted and the policy implications of this sophisticated form of segmentation and targeting. We argue that current consumer policies are inadequate in protecting vulnerable consumers from these marketing techniques, and recent recommendations from the Federal Reserve Bank of the United States, and the Australian Law Reform Commission to allow banks and lenders to ‘pre-screen’ potential customers will exacerbate personal debt levels, rather than reducing them.},
	language = {en},
	number = {4},
	urldate = {2019-08-14},
	journal = {International Journal of Consumer Studies},
	author = {Harrison, Paul and Gray, Charles},
	month = may,
	year = {2010},
	pages = {437--442}
}

@article{haddaway_eviatlas:_2019,
	title = {{EviAtlas}: a tool for visualising evidence synthesis databases},
	volume = {8},
	issn = {2047-2382},
	shorttitle = {{EviAtlas}},
	url = {https://doi.org/10.1186/s13750-019-0167-1},
	doi = {10.1186/s13750-019-0167-1},
	abstract = {Systematic mapping assesses the nature of an evidence base, answering how much evidence exists on a particular topic. Perhaps the most useful outputs of a systematic map are an interactive database of studies and their meta-data, along with visualisations of this database. Despite the rapid increase in systematic mapping as an evidence synthesis method, there is currently a lack of Open Source software for producing interactive visualisations of systematic map databases. In April 2018, as attendees at and coordinators of the first ever Evidence Synthesis Hackathon in Stockholm, we decided to address this issue by developing an R-based tool called EviAtlas, an Open Access (i.e. free to use) and Open Source (i.e. software code is freely accessible and reproducible) tool for producing interactive, attractive tables and figures that summarise the evidence base. Here, we present our tool which includes the ability to generate vital visualisations for systematic maps and reviews as follows: a complete data table; a spatially explicit geographical information system (Evidence Atlas); Heat Maps that cross-tabulate two or more variables and display the number of studies belonging to multiple categories; and standard descriptive plots showing the nature of the evidence base, for example the number of studies published per year or number of studies per country. We believe that EviAtlas will provide a stimulus for the development of other exciting tools to facilitate evidence synthesis.},
	number = {1},
	urldate = {2019-08-14},
	journal = {Environmental Evidence},
	author = {Haddaway, Neal R. and Feierman, Andrew and Grainger, Matthew J. and Gray, Charles T. and Tanriver-Ayder, Ezgi and Dhaubanjar, Sanita and Westgate, Martin J.},
	month = jun,
	year = {2019},
	pages = {22}
}

@misc{uhs_universal_2019,
	title = {Universal {Hint} {System}: {Not} your ordinary walkthrough. {Just} the hints you need.},
	url = {http://www.uhs-hints.com/},
	urldate = {2019-08-13},
	author = {UHS},
	year = {2019}
}

@misc{noauthor_universal_nodate,
	title = {Universal {Hint} {System}: {Not} your ordinary walkthrough. {Just} the hints you need.},
	url = {http://www.uhs-hints.com/},
	urldate = {2019-08-13}
}

@book{wickham_advanced_2014,
	address = {Boca Raton, FL},
	edition = {1 edition},
	title = {Advanced {R}},
	isbn = {978-1-4665-8696-3},
	abstract = {An Essential Reference for Intermediate and Advanced R Programmers Advanced R presents useful tools and techniques for attacking many types of R programming problems, helping you avoid mistakes and dead ends. With more than ten years of experience programming in R, the author illustrates the elegance, beauty, and flexibility at the heart of R.  The book develops the necessary skills to produce quality code that can be used in a variety of circumstances. You will learn:  The fundamentals of R, including standard data types and functions Functional programming as a useful framework for solving wide classes of problems The positives and negatives of metaprogramming  How to write fast, memory-efficient code   This book not only helps current R users become R programmers but also shows existing programmers what’s special about R. Intermediate R programmers can dive deeper into R and learn new strategies for solving diverse problems while programmers from other languages can learn the details of R and understand why R works the way it does.},
	language = {English},
	publisher = {Routledge},
	author = {Wickham, Hadley},
	month = sep,
	year = {2014}
}

@misc{noauthor_manuscript_nodate,
	title = {Manuscript and supplements},
	url = {https://richarddmorey.github.io/Morey_Hoekstra_StatCognition/articles/man_supp.html},
	language = {en},
	urldate = {2019-08-04}
}

@article{belmonte_tangled_2007,
	title = {The tangled web of self-tying knots},
	volume = {104},
	copyright = {© 2007 by The National Academy of Sciences of the USA},
	issn = {0027-8424, 1091-6490},
	url = {https://www.pnas.org/content/104/44/17243},
	doi = {10.1073/pnas.0708150104},
	abstract = {Mathematician Leopold Kronecker stated “God created the integers, all else is the work of man,” alluding to the fact that the natural numbers most likely arose from physical counting, as in one's fingers or goats in the pasture. Topology, however, is arguably a different creature altogether and may have had its own independent origins from the physical world in the ubiquitous knot—something that cannot be undone without using the free ends because the individual strands cannot move through each other (1). One imagines a primordial knot getting tied accidentally in Cro-Magnon times and then tugged at to no avail … perhaps eventually cut. Of course, knots went on to have their uses in early societies, still far from any theoretical considerations but very much related to their ability to bind or secure things such as animals, sails, hair, etc. Knotted strings were also used by the Inca civilization for record-keeping and possibly even communication (2), still a few centuries before Euler began counting bridge-crossings in Königsberg.

The discovery and synthesis of polymers, long-chain molecules such as DNA, has brought a renewed physical relevance and context to knots (3, 4), and with it a new direction of study. For instance, it has been shown by the electrophoresis of loop DNA that knot types from the simplest trefoil to a knot with 10 crossings can occur at the molecular level (5). Although knots were actually tied recently in surfactant nanotubes by micromanipulation (6), molecular knots mostly occur in a spontaneous way, driven by competition between a fluctuating exploration of space due to Brownian motion and the excluded-volume effect (the string cannot pass through itself). Knots are a natural and sometimes irreversible result of this process, and despite scientific study, it is still true that “… a complete statistical mechanical description of …

*E-mail: belmonte\{at\}math.psu.edu},
	language = {en},
	number = {44},
	urldate = {2019-08-02},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Belmonte, Andrew},
	month = oct,
	year = {2007},
	pmid = {17959772},
	pages = {17243--17244}
}

@book{huttermann_devops_2012,
	title = {{DevOps} for {Developers}},
	isbn = {978-1-4302-4570-4},
	abstract = {DevOps for Developers delivers a practical, thorough introduction to approaches, processes and tools to foster collaboration between software development and operations. Efforts of Agile software development often end at the transition phase from development to operations. This book covers the delivery of software, this means “the last mile”, with lean practices for shipping the software to production and making it available to the end users, together with the integration of operations with earlier project phases (elaboration, construction, transition).   DevOps for Developers describes how to streamline the software delivery process and improve the cycle time (that is the time from inception to delivery). It will enable you to deliver software faster, in better quality and more aligned with individual requirements and basic conditions. And above all, work that is aligned with the “DevOps” approach makes even more fun!    Provides patterns and toolchains to integrate software development and operations  Delivers an one-stop shop for kick-starting with DevOps  Provides guidance how to streamline the software delivery process},
	language = {en},
	publisher = {Apress},
	author = {Hüttermann, Michael},
	month = oct,
	year = {2012},
	note = {Google-Books-ID: JfUAkB8AA7EC},
	keywords = {Computers / Information Technology, Computers / Programming / General, Computers / Programming / Open Source, Computers / Software Development \& Engineering / General}
}

@article{ragkhitwetsagul_toxic_2019,
	title = {Toxic {Code} {Snippets} on {Stack} {Overflow}},
	issn = {0098-5589},
	doi = {10.1109/TSE.2019.2900307},
	abstract = {Online code clones are code fragments that are copied from software projects or online sources to Stack Overflow as examples. Due to an absence of a checking mechanism after the code has been copied to Stack Overflow, they can become toxic code snippets, e.g., they suffer from being outdated or violating the original software license. We present a study of online code clones on Stack Overflow and their toxicity by incorporating two developer surveys and a large-scale code clone detection. A survey of 201 high-reputation Stack Overflow answerers (33\% response rate) showed that 131 participants (65\%) have ever been notified of outdated code and 26 of them (20\%) rarely or never fix the code. 138 answerers (69\%) never check for licensing conflicts between their copied code snippets and Stack Overflow?s CC BY-SA 3.0. A survey of 87 Stack Overflow visitors shows that they experienced several issues from Stack Overflow answers: mismatched solutions, outdated solutions, incorrect solutions, and buggy code. 85\% of them are not aware of CC BY-SA 3.0 license enforced by Stack Overflow, and 66\% never check for license conflicts when reusing code snippets. Our clone detection found online clone pairs between 72,365 Java code snippets on Stack Overflow and 111 open source projects in the curated Qualitas corpus. We analysed 2,289 non-trivial online clone candidates. Our investigation revealed strong evidence that 153 clones have been copied from a Qualitas project to Stack Overflow. We found 100 of them (66\%) to be outdated, of which 10 were buggy and harmful for reuse. Furthermore, we found 214 code snippets that could potentially violate the license of their original software and appear 7,112 times in 2,427 GitHub projects.},
	journal = {IEEE Transactions on Software Engineering},
	author = {Ragkhitwetsagul, C. and Krinke, J. and Paixao, M. and Bianco, G. and Oliveto, R.},
	year = {2019},
	keywords = {Cloning, Code Clone Detection, Computer bugs, Licenses, Outdated Code, Programming, Security, Software, Software Licensing, Stack Overflow, Tutorials},
	pages = {1--1}
}

@misc{noauthor_stack_nodate,
	title = {Stack {Overflow} - {Where} {Developers} {Learn}, {Share}, \& {Build} {Careers}},
	url = {https://stackoverflow.com/},
	urldate = {2019-08-02}
}

@inproceedings{ford_paradise_2016,
	address = {New York, NY, USA},
	series = {{FSE} 2016},
	title = {Paradise {Unplugged}: {Identifying} {Barriers} for {Female} {Participation} on {Stack} {Overflow}},
	isbn = {978-1-4503-4218-6},
	shorttitle = {Paradise {Unplugged}},
	url = {http://doi.acm.org/10.1145/2950290.2950331},
	doi = {10.1145/2950290.2950331},
	abstract = {It is no secret that females engage less in programming fields than males. However, in online communities, such as Stack Overflow, this gender gap is even more extreme: only 5.8\% of contributors are female. In this paper, we use a mixed-methods approach to identify contribution barriers females face in online communities. Through 22 semi-structured interviews with a spectrum of female users ranging from non-contributors to a top 100 ranked user of all time, we identified 14 barriers preventing them from contributing to Stack Overflow. We then conducted a survey with 1470 female and male developers to confirm which barriers are gender related or general problems for everyone. Females ranked five barriers significantly higher than males. A few of these include doubts in the level of expertise needed to contribute, feeling overwhelmed when competing with a large number of users, and limited awareness of site features. Still, there were other barriers that equally impacted all Stack Overflow users or affected particular groups, such as industry programmers. Finally, we describe several implications that may encourage increased participation in the Stack Overflow community across genders and other demographics.},
	urldate = {2019-08-02},
	booktitle = {Proceedings of the 2016 24th {ACM} {SIGSOFT} {International} {Symposium} on {Foundations} of {Software} {Engineering}},
	publisher = {ACM},
	author = {Ford, Denae and Smith, Justin and Guo, Philip J. and Parnin, Chris},
	year = {2016},
	note = {event-place: Seattle, WA, USA},
	keywords = {Barriers, Females in Computing, Online Communities, Social Q\&A},
	pages = {846--857}
}

@article{zimmermann_collaborative_nodate,
	title = {Collaborative {Software} {Development} in {Ten} {Years}: {Diversity}, {Tools}, and {Remix} {Culture}},
	abstract = {Over the next ten years, collaboration in software engineering will change in a number of ways and research will need to shift its focus to enable and enhance such collaboration. Specifically, we claim that software in the small will become more popular and even large software will be built by fewer people due to better tools. For large projects, research will need to address the collaboration needs of project members other than just developers, including quality assurance engineers, build engineers, architects, and operations managers. Finally, code reuse and sharing will change as a result of a growing software remix culture, leading to more loosely coupled and indirect collaboration.},
	language = {en},
	author = {Zimmermann, Thomas and Bird, Christian},
	pages = {3}
}

@book{kafka_trial_2005,
	title = {The {Trial}},
	copyright = {Copyrighted. Read the copyright notice inside this book for details.},
	url = {http://www.gutenberg.org/ebooks/7849},
	language = {English},
	urldate = {2019-08-02},
	author = {Kafka, Franz},
	translator = {Wyllie, David},
	month = apr,
	year = {2005},
	keywords = {Social problems -- Fiction}
}

@article{holloway_decision_2019,
	title = {A {Decision} {Tree} {Approach} for {Spatially} {Interpolating} {Missing} {Land} {Cover} {Data} and {Classifying} {Satellite} {Images}},
	volume = {11},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/2072-4292/11/15/1796},
	doi = {10.3390/rs11151796},
	abstract = {Sustainable Development Goals (SDGs) are a set of priorities the United Nations and World Bank have set for countries to reach in order to improve quality of life and environment globally by 2030. Free satellite images have been identified as a key resource that can be used to produce official statistics and analysis to measure progress towards SDGs, especially those that are concerned with the physical environment, such as forest, water, and crops. Satellite images can often be unusable due to missing data from cloud cover, particularly in tropical areas where the deforestation rates are high. There are existing methods for filling in image gaps; however, these are often computationally expensive in image classification or not effective at pixel scale. To address this, we use two machine learning methods\&mdash;gradient boosted machine and random forest algorithms\&mdash;to classify the observed and simulated \&lsquo;missing\&rsquo; pixels in satellite images as either grassland or woodland. We also predict a continuous biophysical variable, Foliage Projective Cover (FPC), which was derived from satellite images, and perform accurate binary classification and prediction using only the latitude and longitude of the pixels. We compare the performance of these methods against each other and inverse distance weighted interpolation, which is a well-established spatial interpolation method. We find both of the machine learning methods, particularly random forest, perform fast and accurate classifications of both observed and missing pixels, with up to 0.90 accuracy for the binary classification of pixels as grassland or woodland. The results show that the random forest method is more accurate than inverse distance weighted interpolation and gradient boosted machine for prediction of FPC for observed and missing data. Based on the case study results from a sub-tropical site in Australia, we show that our approach provides an efficient alternative for interpolating images and performing land cover classifications.},
	language = {en},
	number = {15},
	urldate = {2019-08-02},
	journal = {Remote Sensing},
	author = {Holloway, Jacinta and Helmstedt, Kate J. and Mengersen, Kerrie and Schmidt, Michael},
	month = jan,
	year = {2019},
	keywords = {Sustainable Development Goals, decision trees, gradient boosted machine, inverse distance weighted interpolation, land cover classification, machine learning, missing data, pixel level analysis, random forest, satellite image, spatial data},
	pages = {1796}
}

@article{consalvo_zelda_2003,
	title = {Zelda 64 and {Video} {Game} {Fans}: {A} {Walkthrough} of {Games}, {Intertextuality}, and {Narrative}},
	volume = {4},
	issn = {1527-4764},
	shorttitle = {Zelda 64 and {Video} {Game} {Fans}},
	url = {https://doi.org/10.1177/1527476403253993},
	doi = {10.1177/1527476403253993},
	abstract = {This article argues that to better understand and theorize video games and game playing, it is necessary to study the activities of gamers themselves. This research examines game fans' construction of walkthroughs, which guide other players through the action and story of the game. It is argued that these walkthroughs function as narratives for gamers, which are read intertextually by game fans. Further, gamers should be considered active creators of meaning regarding games, as they exhibit many of the characteristics of traditional media fans, including active reading of the media text, construction of media texts to share with other fans, and knowledge of intertextual relations between various media forms.},
	language = {en},
	number = {3},
	urldate = {2019-08-01},
	journal = {Television \& New Media},
	author = {Consalvo, Mia},
	month = aug,
	year = {2003},
	pages = {321--334}
}

@article{dacrema_are_2019,
	title = {Are {We} {Really} {Making} {Much} {Progress}? {A} {Worrying} {Analysis} of {Recent} {Neural} {Recommendation} {Approaches}},
	shorttitle = {Are {We} {Really} {Making} {Much} {Progress}?},
	url = {http://arxiv.org/abs/1907.06902},
	doi = {10.1145/3298689.3347058},
	abstract = {Deep learning techniques have become the method of choice for researchers working on algorithmic aspects of recommender systems. With the strongly increased interest in machine learning in general, it has, as a result, become difficult to keep track of what represents the state-of-the-art at the moment, e.g., for top-n recommendation tasks. At the same time, several recent publications point out problems in today's research practice in applied machine learning, e.g., in terms of the reproducibility of the results or the choice of the baselines when proposing new models. In this work, we report the results of a systematic analysis of algorithmic proposals for top-n recommendation tasks. Specifically, we considered 18 algorithms that were presented at top-level research conferences in the last years. Only 7 of them could be reproduced with reasonable effort. For these methods, it however turned out that 6 of them can often be outperformed with comparably simple heuristic methods, e.g., based on nearest-neighbor or graph-based techniques. The remaining one clearly outperformed the baselines but did not consistently outperform a well-tuned non-neural linear ranking method. Overall, our work sheds light on a number of potential problems in today's machine learning scholarship and calls for improved scientific practices in this area. Source code of our experiments and full results are available at: https://github.com/MaurizioFD/RecSys2019\_DeepLearning\_Evaluation.},
	urldate = {2019-07-29},
	journal = {arXiv:1907.06902 [cs]},
	author = {Dacrema, Maurizio Ferrari and Cremonesi, Paolo and Jannach, Dietmar},
	month = jul,
	year = {2019},
	note = {arXiv: 1907.06902},
	keywords = {Computer Science - Information Retrieval, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing}
}

@misc{noauthor_help_nodate,
	title = {Help for {Writing} {Unit} {Tests} {Based} on {Function} {Examples}},
	url = {https://rorynolan.github.io/exampletestr/index.html},
	abstract = {Take the examples written in your documentation of functions and
    use them to create shells (skeletons which must be manually completed by
    the user) of test files to be tested with the 'testthat' package. Sort of
    like python 'doctests' for R.},
	language = {en},
	urldate = {2019-07-28}
}

@article{ngugi_multiple_2011,
	title = {Multiple {Treatment} {Meta}-{Analysis} of {Products} {Evaluated} for {Control} of {Fire} {Blight} in the {Eastern} {United} {States}},
	volume = {101},
	issn = {0031-949X},
	url = {https://apsjournals.apsnet.org/doi/abs/10.1094/PHYTO-08-10-0221},
	doi = {10.1094/PHYTO-08-10-0221},
	abstract = {The aim of this analysis was to estimate the effect sizes and consistency of products evaluated for fire blight control in the eastern United States over the last decade. Because only 3\% of the 69 studies published from 2000 to 2008 explicitly presented a measure of within-study variability, a method for estimating the least significant difference (LSD) and, hence the sampling variance, for studies with at least two significant mean separations in the presented mean multiple comparisons was developed. Lin's concordance analysis indicated that the estimated LSD was an accurate predictor of the actual LSD based on 35 studies in a calibration evaluation (ρc = 0.997). Separate multi-treatment random-effects meta-analyses were performed for three control categories: antibiotics, biological control, and plant defense-activating products and mean log response ratios relative to the nontreated controls () were computed for each treatment and then back-transformed to obtain the mean percent disease control. None of the products evaluated performed as well as streptomycin, the standard product for fire blight control, for which the mean disease control was 68.6\%. As a group, experimental antibiotics provided the best fire blight control with mean effect sizes ranging from 59.7 to 61.7\%. Among the biological controls, the best control was noted for treatments combining the antibiotic streptomycin with a product based on Pantoea agglomerans (55.0\% mean disease reduction) or Bacillus subtilis (53.9\%). Mean disease control was 31.9, 25.7, and 22.6\%, respectively, for products based on B. subtilis, Pantoea agglomerans, and Pseudomonas fluorescens without an antibiotic, suggesting that the higher efficacy of the combination treatments was due to the antibiotic. Among the plant defense-activating products, prohexadione calcium had the highest and most consistent effect size (50.7\% control), while other products provided modest mean disease control of between 6.1 and 25.8\%. Percent control values were significantly moderated by study location and cultivar used in the study, and were smaller, but more variable, when products were tested under high disease intensity compared with low disease intensity. Results indicate that wide-scale use of biological control and plant defense-activating products in the eastern United States is likely to remain low.},
	number = {5},
	urldate = {2019-07-22},
	journal = {Phytopathology},
	author = {Ngugi, H. K. and Lehman, B. L. and Madden, L. V.},
	month = jan,
	year = {2011},
	pages = {512--522}
}

@misc{wickham_usethis:_2019,
	title = {usethis: {Automate} {Package} and {Project} {Setup}},
	url = {https://CRAN.R-project.org/package=usethis},
	author = {Wickham, Hadley and Bryan, Jennifer},
	year = {2019}
}

@book{bryan_happy_2019,
	title = {Happy {Git} with {R}},
	url = {https://happygitwithr.com/resources.html},
	author = {Bryan, Jennifer},
	year = {2019}
}

@book{stodden_implementing_2014,
	title = {Implementing {Reproducible} {Research}},
	isbn = {978-1-4665-6159-5},
	abstract = {In computational science, reproducibility requires that researchers make code and data available to others so that the data can be analyzed in a similar manner as in the original publication. Code must be available to be distributed, data must be accessible in a readable format, and a platform must be available for widely distributing the data and code. In addition, both data and code need to be licensed permissively enough so that others can reproduce the work without a substantial legal burden. Implementing Reproducible Research covers many of the elements necessary for conducting and distributing reproducible research. It explains how to accurately reproduce a scientific result.   Divided into three parts, the book discusses the tools, practices, and dissemination platforms for ensuring reproducibility in computational science. It describes:   Computational tools, such as Sweave, knitr, VisTrails, Sumatra, CDE, and the Declaratron system Open source practices, good programming practices, trends in open science, and the role of cloud computing in reproducible research Software and methodological platforms, including open source software packages, RunMyCode platform, and open access journals  Each part presents contributions from leaders who have developed software and other products that have advanced the field. Supplementary material is available at www.ImplementingRR.org.},
	language = {en},
	publisher = {CRC Press},
	author = {Stodden, Victoria and Leisch, Friedrich and Peng, Roger D.},
	month = apr,
	year = {2014},
	note = {Google-Books-ID: JcmSAwAAQBAJ},
	keywords = {Computers / Mathematical \& Statistical Software, Mathematics / Probability \& Statistics / General, Science / Life Sciences / Biology}
}

@article{unertl_traversing_2010,
	title = {Traversing the many paths of workflow research: developing a conceptual framework of workflow terminology through a systematic literature review},
	volume = {17},
	issn = {1067-5027, 1527-974X},
	shorttitle = {Traversing the many paths of workflow research},
	url = {https://academic.oup.com/jamia/article-lookup/doi/10.1136/jamia.2010.004333},
	doi = {10.1136/jamia.2010.004333},
	abstract = {The objective of this review was to describe methods used to study and model workﬂow. The authors included studies set in a variety of industries using qualitative, quantitative and mixed methods. Of the 6221 matching abstracts, 127 articles were included in the ﬁnal corpus. The authors collected data from each article on researcher perspective, study type, methods type, speciﬁc methods, approaches to evaluating quality of results, deﬁnition of workﬂow and dependent variables. Ethnographic observation and interviews were the most frequently used methods. Long study durations revealed the large time commitment required for descriptive workﬂow research. The most frequently discussed technique for evaluating quality of study results was triangulation. The deﬁnition of the term “workﬂow” and choice of methods for studying workﬂow varied widely across research areas and researcher perspectives. The authors developed a conceptual framework of workﬂowrelated terminology for use in future research and present this model for use by other researchers.},
	language = {en},
	number = {3},
	urldate = {2019-07-20},
	journal = {Journal of the American Medical Informatics Association},
	author = {Unertl, Kim M and Novak, Laurie L and Johnson, Kevin B and Lorenzi, Nancy M},
	month = may,
	year = {2010},
	pages = {265--273}
}

@article{unertl_traversing_2010-1,
	title = {Traversing the many paths of workflow research: developing a conceptual framework of workflow terminology through a systematic literature review},
	volume = {17},
	issn = {1067-5027},
	shorttitle = {Traversing the many paths of workflow research},
	url = {https://academic.oup.com/jamia/article/17/3/265/738765},
	doi = {10.1136/jamia.2010.004333},
	abstract = {Abstract.  The objective of this review was to describe methods used to study and model workflow. The authors included studies set in a variety of industries us},
	language = {en},
	number = {3},
	urldate = {2019-07-20},
	journal = {Journal of the American Medical Informatics Association},
	author = {Unertl, Kim M. and Novak, Laurie L. and Johnson, Kevin B. and Lorenzi, Nancy M.},
	month = may,
	year = {2010},
	pages = {265--273}
}

@article{kraker_vienna_2016,
	title = {The {Vienna} {Principles}: {A} {Vision} for {Scholarly} {Communication} in the 21st {Century}},
	volume = {69},
	issn = {1022-2588},
	shorttitle = {The {Vienna} {Principles}},
	url = {https://journals.univie.ac.at/index.php/voebm/article/view/2037},
	doi = {10.31263/voebm.v69i3.1733},
	abstract = {Zur Zeit gibt es starke Bemühungen, die offensichtlichen Defizite des wissenschaftlichen Kommunikationssystems zu beheben. Open Science hat das Potenzial, die Produktion und Verbreitung von wissenschaftlichem Wissen positiv zu verändern; es existiert aber keine gemeinsam geteilte Vision, die das System wissenschaftlicher Kommunikation beschreibt, welches wir erschaffen wollen. Zwischen April 2015 und Juni 2016 trafen sich in Wien die Mitglieder der Open Access Network Austria (OANA) Arbeitsgruppe "Open Access and Scholarly Communication", um diese Angelegenheit zu diskutieren. Das Hauptergebnis unserer Überlegungen sind zwölf Prinzipien, die die Eckpfeiler eines künftigen wissenschaftlichen Kommunikationssystems dedarstellen. Diese Prinzipien sollen einen kohärenten Bezugsrahmen für die Debatte zur Verbesserung des derzeitigen Systems liefern. Mit diesem Dokument hoffen wir, eine breite Diskussion über eine gemeinsame Vision für die wissenschaftliche Kommunikation im 21. Jahrhundert anzustoßen.},
	language = {en},
	number = {3-4},
	urldate = {2019-07-19},
	journal = {Mitteilungen der Vereinigung Österreichischer Bibliothekarinnen und Bibliothekare},
	author = {Kraker, Peter and Dörler, Daniel and Ferus, Andreas and Gutounig, Robert and Heigl, Florian and Kaier, Christian and Rieck, Katharina and Šimukovič, Elena and Vignoli, Michela},
	month = dec,
	year = {2016},
	pages = {436--446}
}

@misc{noauthor_vienna_nodate,
	title = {Vienna {Principles} a vision for scholarly communication},
	url = {https://viennaprinciples.org/},
	urldate = {2019-07-19}
}

@article{davey2018the,
	title = {The {Homomorphism} {Lattice} {Induced} by a {Finite} {Algebra}},
	volume = {35},
	issn = {0167-8094},
	url = {https://doi.org/10.1007/s11083-017-9426-3},
	doi = {10.1007/s11083-017-9426-3},
	abstract = {Each finite algebra A induces a lattice LA via the quasi-order → on the finite members of the variety generated by A, where B →C if there exists a homomorphism from B to C. In this paper, we introduce the question: ‘Which lattices arise as the homomorphism lattice LA induced by a finite algebra A?’ Our main result is that each finite distributive lattice arises as LQ, for some quasi-primal algebra Q. We also obtain representations of some other classes of lattices as homomorphism lattices, including all finite partition lattices, all finite subspace lattices and all lattices of the form L ⊕1, where L is an interval in the subgroup lattice of a finite group.},
	number = {2},
	journal = {Order},
	author = {Davey, Brian A. and Gray, Charles T. and Pitkethly, Jane G.},
	year = {2018},
	pages = {193--214}
}

@incollection{martin-lof_constructive_1982,
	series = {Logic, {Methodology} and {Philosophy} of {Science} {VI}},
	title = {Constructive {Mathematics} and {Computer} {Programming}},
	volume = {104},
	url = {http://www.sciencedirect.com/science/article/pii/S0049237X09701892},
	abstract = {This chapter discusses that relating constructive mathematics to computer programming seems to be beneficial. Among the benefits to be derived by constructive mathematics from its association with computer programming, one is that you see immediately why you cannot rely upon the law of excluded middle: its uninhibited use would lead to programs that one did not know how to execute. By choosing to program in a formal language for constructive mathematics, like the theory of types, one gets access to the conceptual apparatus of pure mathematics, neglecting those parts that depend critically on the law of excluded middle, whereas even the best high level programming languages so far designed are wholly inadequate as mathematical languages. The virtue of a machine code is that a program written in it can be directly read and executed by the machine. The distinction between low and high level programming languages is of course relative to the available hardware. It may well be possible to turn what is now regarded as a high level programming language into machine code by the invention of new hardware.},
	urldate = {2019-07-17},
	booktitle = {Studies in {Logic} and the {Foundations} of {Mathematics}},
	publisher = {Elsevier},
	author = {Martin-Löf, Per},
	editor = {Cohen, L. Jonathan and Łoś, Jerzy and Pfeiffer, Helmut and Podewski, Klaus-Peter},
	month = jan,
	year = {1982},
	doi = {10.1016/S0049-237X(09)70189-2},
	pages = {153--175}
}

@misc{katz_super_2019,
	title = {Super {RSEs}: {Combining} research and service in three dimensions of {Research} {Software} {Engineering}},
	shorttitle = {Super {RSEs}},
	url = {https://danielskatzblog.wordpress.com/2019/07/12/},
	abstract = {We typically think of Research Software Engineers (RSEs) as working to support one or more researchers, either one-on-one or through a university’s centra…},
	language = {en},
	urldate = {2019-07-16},
	journal = {Daniel S. Katz's blog},
	author = {Katz, Daniel S. and McHenry, Kenton},
	month = jul,
	year = {2019}
}

@article{rodriguez-sanchez_ciencia_2016,
	title = {Ciencia reproducible: qué, por qué, cómo},
	volume = {25},
	copyright = {Las obras que se publican en esta revista están sujetas a los siguientes términos:   1. Las obras se publican en edición electrónica, en acceso abierto y bajo una licencia  Creative Commons Attribution-Non Comercial  License 3.0 . Se permite a otros distribuir, copiar o adaptar las obras así como crear obras derivadas siempre que se cite la autoría del trabajo y su  publicación inicial en esta revista. No se permite el uso de estas obras ni de sus derivadas con fines comerciales.   2. Los autores conservan los derechos de autor, garantizan a la revista el derecho de ser la primera publicación del trabajo y están de acuerdo con la licencia de uso utilizada por la revista, con las condiciones de autoarchivo y con la política de acceso abierto.   3. Se permite y anima a los autores a difundir sus trabajos electrónicamente (por ejemplo, en repositorios institucionales o en su propio sitio web) ya que puede dar lugar a intercambios productivos, así como a una mayor citación de los trabajos publicados.},
	issn = {1697-2473},
	shorttitle = {Ciencia reproducible},
	url = {https://www.revistaecosistemas.net/index.php/ecosistemas/article/view/1178},
	doi = {10.7818/re.2014.25-2.00},
	abstract = {Rodríguez-Sánchez, F., Pérez-Luque, A.J. Bartomeus, I., Varela, S. 2016. Ciencia reproducible: qué, por qué, cómo. Ecosistemas 25(2): 83-92. Doi.: 10.7818/ECOS.2016.25-2.11
La inmensa mayoría de los estudios científicos no son reproducibles: resulta muy difícil, si no imposible, trazar todo el proceso de análisis y obtención de resultados a partir de un conjunto de datos – incluso tratándose de los mismos investigadores. La trazabilidad y reproducibilidad de los resultados son sin embargo condiciones inherentes a la ciencia de calidad, y un requisito cada vez más frecuente por parte de revistas y organismos financiadores de la investigación. Los estudios científicos reproducibles incluyen código informático capaz de recrear todos los resultados a partir de los datos originales. De esta manera el proceso de análisis queda perfectamente registrado, se reduce drásticamente el riesgo de errores, y se facilita la reutilización de código para otros análisis. Pero la ciencia reproducible no sólo acelera el progreso científico sino que también reporta múltiples beneficios para el investigador como el ahorro de tiempo y esfuerzo, o el incremento de la calidad e impacto de sus publicaciones. En este artículo explicamos en qué consiste la reproducibilidad, por qué es necesaria en ciencia, y cómo podemos hacer ciencia reproducible. Presentamos una serie de recomendaciones y herramientas para el manejo y análisis de datos, control de versiones de archivos, organización de ficheros y manejo de programas informáticos que nos permiten desarrollar flujos de trabajo reproducibles en el contexto actual de la ecología., Rodríguez-Sánchez, F., Pérez-Luque, A.J. Bartomeus, I., Varela, S. 2016. Reproducible science: what, why, how. Ecosistemas 25(2): 83-92. Doi.: 10.7818/ECOS.2016.25-2.11
Most scientific papers are not reproducible: it is really hard, if not impossible, to understand how results are derived from data, and being able to regenerate them in the future (even by the same researchers). However, traceability and reproducibility of results are indispensable elements of high-quality science, and an increasing requirement of many journals and funding sources. Reproducible studies include code able to regenerate results from the original data. This practice not only provides a perfect record of the whole analysis but also reduces the probability of errors and facilitates code reuse, thus accelerating scientific progress. But doing reproducible science also brings many benefits to the individual researcher, including saving time and effort, improved collaborations, and higher quality and impact of final publications. In this article we introduce reproducible science, why it is important, and how we can improve the reproducibility of our work. We introduce principles and tools for data management, analysis, version control, and software management that help us achieve reproducible workflows in the context of ecology.},
	language = {es},
	number = {2},
	urldate = {2019-07-17},
	journal = {Revista Ecosistemas},
	author = {Rodriguez-Sanchez, Francisco and Pérez-Luque, Antonio Jesús and Bartomeus, Ignasi and Varela, Sara},
	month = jul,
	year = {2016},
	keywords = {R, análisis de datos, ciencia abierta, data analysis, ecoinformatics, ecoinformática, ecology, ecología, open science, programación, programming, reproducibilidad, reproducibility},
	pages = {83--92--92}
}

@article{gray_truth_2019,
	title = {Truth, {Proof}, and {Reproducibility}: {There}'s no counter-attack for the codeless},
	copyright = {All rights reserved},
	shorttitle = {Truth, {Proof}, and {Reproducibility}},
	url = {http://arxiv.org/abs/1907.05947},
	abstract = {Current concerns about reproducibility in many research communities can be traced back to a high value placed on empirical reproducibility of the physical details of scientific experiments and observations. For example, the detailed descriptions by 17th century scientist Robert Boyle of his vacuum pump experiments are often held to be the ideal of reproducibility as a cornerstone of scientific practice. Victoria Stodden has claimed that the computer is an analog for Boyle's pump -- another kind of scientific instrument that needs detailed descriptions of how it generates results. In the place of Boyle's hand-written notes, we now expect code in open source programming languages to be available to enable others to reproduce and extend computational experiments. In this paper we show that there is another genealogy for reproducibility, starting at least from Euclid, in the production of proofs in mathematics. Proofs have a distinctive quality of being necessarily reproducible, and are the cornerstone of mathematical science. However, the task of the modern mathematical scientist has drifted from that of blackboard rhetorician, where the craft of proof reigned, to a scientific workflow that now more closely resembles that of an experimental scientist. So, what is proof in modern mathematics? And, if proof is unattainable in other fields, what is due scientific diligence in a computational experimental environment? How do we measure truth in the context of uncertainty? Adopting a manner of Lakatosian conversant conjecture between two mathematicians, we examine how proof informs our practice of computational statistical inquiry. We propose that a reorientation of mathematical science is necessary so that its reproducibility can be readily assessed.},
	urldate = {2019-07-17},
	journal = {arXiv:1907.05947 [math]},
	author = {Gray, Charles T. and Marwick, Ben},
	month = jul,
	year = {2019},
	note = {arXiv: 1907.05947},
	keywords = {06-06, Mathematics - History and Overview}
}

@misc{limited_test-driven_nodate,
	title = {Test-{Driven} {Development}: {A} {Review}},
	shorttitle = {Test-{Driven} {Development}},
	url = {http://www.tdda.info/test-driven-development-a-review.html},
	abstract = {Since a key motivation for developing test-driven data analysis (TDDA) has been test-driven development (TDD), we need to conduct a lightning tour of TDD before outlining how we see TDDA developing. If you are already familiar with test-driven development, this may not contain too much that is new for you …},
	language = {en},
	urldate = {2019-07-16},
	journal = {Test-Driven Data Analysis},
	author = {Limited, Stochastic Solutions}
}

@misc{limited_test-driven_nodate-1,
	title = {Test-{Driven} {Data} {Analysis}},
	url = {http://www.tdda.info},
	language = {en},
	urldate = {2019-07-16},
	journal = {Test-Driven Data Analysis},
	author = {Limited, Stochastic Solutions}
}

@book{mackenzie_mechanizing_2004,
	address = {Cambridge, Mass. London},
	title = {Mechanizing {Proof}: {Computing}, {Risk}, and {Trust}},
	isbn = {978-0-262-63295-9},
	shorttitle = {Mechanizing {Proof}},
	abstract = {This series presents recent research on the effects of taxation and government expenditure programs on economic performance and analyses of the effects of potential tax reforms. The research results appear in a form that is accessible to tax practitioners and policymakers.},
	language = {English},
	publisher = {MIT Press},
	author = {MacKenzie, Donald},
	month = jan,
	year = {2004},
	keywords = {greg}
}

@misc{wyatt_research_2019,
	title = {Research {Software} {Engineers} {Association}},
	url = {https://rse.ac.uk/},
	abstract = {Welcome to the UK RSE Association. We are an association working to create a community and raise awareness of the UK's Research Software Engineers.},
	language = {en-US},
	urldate = {2019-07-16},
	journal = {s},
	author = {Wyatt, Claire},
	year = {2019}
}

@article{devezer_scientific_2019,
	title = {Scientific discovery in a model-centric framework: {Reproducibility}, innovation, and epistemic diversity},
	volume = {14},
	issn = {1932-6203},
	shorttitle = {Scientific discovery in a model-centric framework},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0216125},
	doi = {10.1371/journal.pone.0216125},
	abstract = {Consistent confirmations obtained independently of each other lend credibility to a scientific result. We refer to results satisfying this consistency as reproducible and assume that reproducibility is a desirable property of scientific discovery. Yet seemingly science also progresses despite irreproducible results, indicating that the relationship between reproducibility and other desirable properties of scientific discovery is not well understood. These properties include early discovery of truth, persistence on truth once it is discovered, and time spent on truth in a long-term scientific inquiry. We build a mathematical model of scientific discovery that presents a viable framework to study its desirable properties including reproducibility. In this framework, we assume that scientists adopt a model-centric approach to discover the true model generating data in a stochastic process of scientific discovery. We analyze the properties of this process using Markov chain theory, Monte Carlo methods, and agent-based modeling. We show that the scientific process may not converge to truth even if scientific results are reproducible and that irreproducible results do not necessarily imply untrue results. The proportion of different research strategies represented in the scientific population, scientists’ choice of methodology, the complexity of truth, and the strength of signal contribute to this counter-intuitive finding. Important insights include that innovative research speeds up the discovery of scientific truth by facilitating the exploration of model space and epistemic diversity optimizes across desirable properties of scientific discovery.},
	language = {en},
	number = {5},
	urldate = {2019-07-14},
	journal = {PLOS ONE},
	author = {Devezer, Berna and Nardin, Luis G. and Baumgaertner, Bert and Buzbas, Erkan Ozge},
	month = may,
	year = {2019},
	keywords = {Markov models, Replication studies, Reproducibility, Scientists, Species diversity, Statistical data, Statistical theories, Stochastic processes},
	pages = {e0216125}
}

@misc{hester_covr_2018,
	title = {covr: {Test} {Coverage} for {Packages}},
	url = {https://CRAN.R-project.org/package=covr},
	author = {Hester, Jim},
	year = {2018},
	keywords = {measured.}
}

@misc{stodden_what_2014,
	title = {What scientific idea is ready for retirement?},
	url = {https://www.edge.org/response-detail/25340.%202014.},
	author = {Stodden, Victoria},
	year = {2014}
}

@misc{wickham_testthat_2011,
	title = {testthat: {Get} {Started} with {Testing}},
	abstract = {Software testing is important, but many of us don’t do it because it is frustrating and boring. testthat is a new testing framework for R that is easy learn and use, and integrates with your existing workﬂow. This paper shows how, with illustrations from existing packages.},
	author = {Wickham, Hadley},
	year = {2011}
}

@book{merton_social_1996,
	title = {On {Social} {Structure} and {Science}},
	isbn = {978-0-226-52071-1},
	abstract = {Robert K. Merton is unarguably one of the most influential sociologists of his time. A figure whose wide-ranging theoretical and methodological contributions have become fundamental to the field, Merton is best known for introducing such concepts and procedures as unanticipated consequences, self-fulfilling prophecies, focused group interviews, middle-range theory, opportunity structure, and analytic paradigms. This definitive compilation encompasses the breadth and brilliance of his works, from the earliest to the most recent. Merton's foundational writings on social structure and process, on the sociology of science and knowledge, and on the discipline and trajectory of sociology itself are all powerfully represented, as are his autobiographical insights in a fascinating coda. Anchored by Piotr Sztompka's contextualizing introduction, Merton's vast oeuvre emerges as a dynamic and profoundly coherent system of thought, a constant source of vitality and renewal for present and future sociology.},
	language = {English},
	publisher = {University of Chicago Press},
	author = {Merton, Robert K.},
	month = sep,
	year = {1996},
	note = {Google-Books-ID: j94XiVDwAZEC },
	keywords = {Science / General, Science / Philosophy \& Social Aspects, Science / Philosophy ࡱ{\textbackslash}\& Social Aspects}
}

@article{zeileis_cran_2005-1,
	title = {{CRAN} {Task} {Views}},
	volume = {5},
	url = {https://CRAN.R-project.org/doc/Rnews/},
	number = {1},
	journal = {R News},
	author = {Zeileis, Achim},
	year = {2005},
	pages = {39--40}
}

@misc{wickham_ggplot2_2016,
	title = {ggplot2: {Elegant} {Graphics} for {Data} {Analysis}},
	isbn = {978-3-319-24277-4},
	url = {https://ggplot2.tidyverse.org},
	publisher = {Springer-Verlag New York},
	author = {Wickham, Hadley},
	year = {2016}
}

@misc{wickham_tidyverse_2017,
	title = {tidyverse: {Easily} {Install} and {Load} the '{Tidyverse}'},
	url = {https://CRAN.R-project.org/package=tidyverse},
	author = {Wickham, Hadley},
	year = {2017}
}

@misc{metaverse_2019,
	title = {metaverse: {Workflows} for evidence synthesis projects},
	url = {(link: https://github.com/rmetaverse/metaverse) github.com/rmetaverse/met…},
	author = {Westgate, Martin and Barrett, Malcolm and Grames, Eliza and Gray, Charles and Hamilton, W. Kyle and Kothe, Emily and McGuinness, Luke and O'Dea, Rose and Sanchez-Tojar, Alfredo and Schermann, Michael},
	year = {2019},
	note = {R package version 0.0.1 }
}

@misc{robinson_broom_2019,
	title = {broom: {Convert} {Statistical} {Analysis} {Objects} into {Tidy} {Tibbles}},
	url = {https://CRAN.R-project.org/package=broom},
	author = {Robinson, David and Hayes, Alex},
	year = {2019}
}

@misc{marwick_rrtools_2018,
	title = {rrtools: {Creates} a reproducible research compendium},
	url = {https://github.com/benmarwick/rrtools},
	author = {Marwick, Ben},
	year = {2018},
	keywords = {measured.}
}

@misc{hester_covr_2016,
	title = {covr: {Bringing} test coverage to {R}},
	shorttitle = {covr},
	url = {https://www.rstudio.com/resources/webinars/covr-bringing-test-coverage-to-r/},
	abstract = {covr: Bringing test coverage to R Download Materials Description Code coverage records whether or not each line of code in a package is executed by the package’s tests. While it does not check whether a given program or test executes properly it does reveal areas of the code which are untested.

Coverage has},
	language = {English},
	urldate = {2019-06-13},
	author = {Hester, Jim},
	month = jan,
	year = {2016}
}

@article{stodden_setting_2013,
	title = {"{Setting} the {Default} to {Reproducible}” in {Computational} {Science} {Research}},
	volume = {46},
	number = {5},
	journal = {SIAM News},
	author = {Stodden, Victoria and Borwein, Jonathan and Bailey, David H.},
	year = {2013}
}

@article{wallach_reproducible_2018,
	title = {Reproducible research practices, transparency, and open access data in the biomedical literature, 2015–2017},
	volume = {16},
	issn = {1545-7885},
	url = {https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.2006930},
	doi = {10.1371/journal.pbio.2006930},
	abstract = {Currently, there is a growing interest in ensuring the transparency and reproducibility of the published scientific literature. According to a previous evaluation of 441 biomedical journals articles published in 2000–2014, the biomedical literature largely lacked transparency in important dimensions. Here, we surveyed a random sample of 149 biomedical articles published between 2015 and 2017 and determined the proportion reporting sources of public and/or private funding and conflicts of interests, sharing protocols and raw data, and undergoing rigorous independent replication and reproducibility checks. We also investigated what can be learned about reproducibility and transparency indicators from open access data provided on PubMed. The majority of the 149 studies disclosed some information regarding funding (103, 69.1\% [95\% confidence interval, 61.0\% to 76.3\%]) or conflicts of interest (97, 65.1\% [56.8\% to 72.6\%]). Among the 104 articles with empirical data in which protocols or data sharing would be pertinent, 19 (18.3\% [11.6\% to 27.3\%]) discussed publicly available data; only one (1.0\% [0.1\% to 6.0\%]) included a link to a full study protocol. Among the 97 articles in which replication in studies with different data would be pertinent, there were five replication efforts (5.2\% [1.9\% to 12.2\%]). Although clinical trial identification numbers and funding details were often provided on PubMed, only two of the articles without a full text article in PubMed Central that discussed publicly available data at the full text level also contained information related to data sharing on PubMed; none had a conflicts of interest statement on PubMed. Our evaluation suggests that although there have been improvements over the last few years in certain key indicators of reproducibility and transparency, opportunities exist to improve reproducible research practices across the biomedical literature and to make features related to reproducibility more readily visible in PubMed.},
	language = {English},
	number = {11},
	urldate = {2019-07-03},
	journal = {PLOS Biology},
	author = {Wallach, Joshua D. and Boyack, Kevin W. and Ioannidis, John P. A.},
	month = nov,
	year = {2018},
	keywords = {Government funding of science, Open access publishing, Open science, Replication studies, Reproducibility, Scientific publishing, Sequence databases, Systematic reviews},
	pages = {e2006930}
}
@article{parker_opinionated_2017,
	title = {Opinionated analysis development},
	doi = {10.7287/peerj.preprints.3210v1},
	abstract = {Traditionally, statistical training has focused primarily on mathematical derivations and proofs of statistical tests. The process of developing the technical artifact—that is, the paper, dashboard, or other deliverable—is much less frequently taught, presumably because of an aversion to cookbookery or prescribing specific software choices. In this paper I argue that it’s critical to teach analysts how to go about developing an analysis in order to maximize the probability that their analysis is reproducible, accurate, and collaborative. A critical component of this is adopting a blameless postmortem culture. By encouraging the use of and fluency in tooling that implements these opinions, as well as a blameless way of correcting course as analysts encounter errors, we as a community can foster the growth of processes that fail the practitioners as infrequently as possible.},
	language = {English},
	urldate = {2018-11-03},
	journal = {preprint},
	author = {Parker, Hilary},
	year = {2017},
	keywords = {dumpsterfire, measured.}
}

@inproceedings{brown2013partial,
	title = {Partial unpacking and indirect proofs: {A} study of students’ productive use of the symbolic proof scheme},
	volume = {2},
	booktitle = {Proceedings of the 16th {Annual} {Conference} on {Research} in {Undergraduate} {Mathematics} {Education}},
	author = {Brown, Stacy},
	year = {2013},
	pages = {47--54}
}

@article{murray_how_2005,
	title = {How to {Accuse} the {Other} {Guy} of {Lying} with {Statistics}},
	volume = {20},
	issn = {0883-4237},
	url = {https://www.jstor.org/stable/20061179},
	abstract = {We've known how to lie with statistics for 50 years now. What we really need are theory and praxis for accusing someone else of lying with statistics. The author's experience with the response to "The Bell Curve" has led him to suspect that such a formulation already exists, probably imparted during a secret initiation for professors in the social sciences. This article represents his best attempt to reconstruct what must be in it.},
	number = {3},
	urldate = {2019-06-27},
	journal = {Statistical Science},
	author = {Murray, Charles},
	year = {2005},
	pages = {239--241}
}

@article{leveque_reproducible_2012,
	title = {Reproducible research for scientific computing: {Tools} and strategies for changing the culture},
	volume = {14},
	doi = {10.1109/mcse.2012.38},
	journal = {Comput Sci Eng},
	author = {LeVeque, R. J. and Mitchell, I. M. and Stodden, V.},
	year = {2012}
}

@article{peng_reproducible_2011-1,
	title = {Reproducible {Research} in {Computational} {Science}},
	volume = {334},
	issn = {0036-8075, 1095-9203},
	doi = {10.1126/science.1213847},
	number = {6060},
	journal = {Science},
	author = {Peng, R. D.},
	month = dec,
	year = {2011},
	pages = {1226--1227}
}

@book{haack_defending_2011,
	title = {Defending {Science} - within {Reason}: {Between} {Scientism} {And} {Cynicism}},
	isbn = {978-1-61592-168-3},
	shorttitle = {Defending {Science} - within {Reason}},
	abstract = {Sweeping in scope, penetrating in analysis, and generously illustrated with examples from the history of science, this new and original approach to familiar questions about scientific evidence and method tackles vital questions about science and its place in society. Avoiding the twin pitfalls of scientism and cynicism, noted philosopher Susan Haack argues that, fallible and flawed as they are, the natural sciences have been among the most successful of human enterprises-valuable not only for the vast, interlocking body of knowledge they have discovered, and not only for the technological advances that have improved our lives, but as a manifestation of the human talent for inquiry at its imperfect but sometimes remarkable best.This wide-ranging, trenchant, and illuminating book explores the complexities of scientific evidence, and the multifarious ways in which the sciences have refined and amplified the methods of everyday empirical inquiry; articulates the ways in which the social sciences are like the natural sciences, and the ways in which they are different; disentangles the confusions of radical rhetoricians and cynical sociologists of science; exposes the evasions of apologists for religious resistance to scientific advances; weighs the benefits and the dangers of technology; tracks the efforts of the legal system to make the best use of scientific testimony; and tackles predictions of the eventual culmination, or annihilation, of the scientific enterprise.Writing with verve and wry humor, in a witty, direct, and accessible style, Haack takes readers beyond the "Science Wars" to a balanced understanding of the value, and the limitations, of the scientific enterprise.},
	publisher = {Prometheus Books},
	author = {Haack, Susan},
	month = mar,
	year = {2011},
	note = {Google-Books-ID: RhXxaPTc\_EYC },
	keywords = {Science / Philosophy {\textbackslash}{\textbackslash}\& Social Aspects, Science / Research ࡱ{\textbackslash}\& Methodology}
}

@book{pickering2010mangle,
	title = {The mangle of practice: {Time}, agency, and science},
	publisher = {University of Chicago Press},
	author = {Pickering, Andrew},
	year = {2010}
}

@book{shapin2011leviathan,
	title = {Leviathan and the air-pump: {Hobbes}, {Boyle}, and the experimental life ({New} in paper)},
	volume = {32},
	publisher = {Princeton University Press},
	author = {Shapin, Steven and Schaffer, Simon},
	year = {2011}
}

@article{head_extent_2015,
	title = {The {Extent} and {Consequences} of {P}-{Hacking} in {Science}},
	volume = {13},
	issn = {1545-7885},
	url = {https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.1002106},
	doi = {10.1371/journal.pbio.1002106},
	abstract = {A focus on novel, confirmatory, and statistically significant results leads to substantial bias in the scientific literature. One type of bias, known as “p-hacking,” occurs when researchers collect or select data or statistical analyses until nonsignificant results become significant. Here, we use text-mining to demonstrate that p-hacking is widespread throughout science. We then illustrate how one can test for p-hacking when performing a meta-analysis and show that, while p-hacking is probably common, its effect seems to be weak relative to the real effect sizes being measured. This result suggests that p-hacking probably does not drastically alter scientific consensuses drawn from meta-analyses.},
	number = {3},
	urldate = {2019-06-22},
	journal = {PLOS Biology},
	author = {Head, Megan L. and Holman, Luke and Lanfear, Rob and Kahn, Andrew T. and Jennions, Michael D.},
	month = mar,
	year = {2015},
	keywords = {Binomials, Medicine and health sciences, Meta-analysis, Psychology, Publication ethics, Research validity, Statistical data, Test statistics},
	pages = {e1002106}
}

@article{noauthor_reproducible_2019,
	title = {Reproducible ({Data}) {Science} with {Docker} and {R} · {Samples} of {Thoughts} - {Samples} of {Thoughts}},
	url = {https://samples-of-thoughts.com/post/reproducible-data-science/},
	abstract = {In my data team at work, we’ve been using Docker for a while now. At least, the engineers in our team have been using it, we data scientists have been very relu},
	language = {English},
	urldate = {2019-06-18},
	month = jun,
	year = {2019}
}

@book{lakatos_proofs_2015,
	address = {Cambridge},
	edition = {Reissue edition},
	title = {Proofs and {Refutations}: {The} {Logic} of {Mathematical} {Discovery}},
	isbn = {978-1-107-53405-6},
	shorttitle = {Proofs and {Refutations}},
	abstract = {Imre Lakatos's Proofs and Refutations is an enduring classic, which has never lost its relevance. Taking the form of a dialogue between a teacher and some students, the book considers various solutions to mathematical problems and, in the process, raises important questions about the nature of mathematical discovery and methodology. Lakatos shows that mathematics grows through a process of improvement by attempts at proofs and critiques of these attempts, and his work continues to inspire mathematicians and philosophers aspiring to develop a philosophy of mathematics that accounts for both the static and the dynamic complexity of mathematical practice. With a specially commissioned Preface written by Paolo Mancosu, this book has been revived for a new generation of readers.},
	publisher = {Cambridge University Press},
	author = {Lakatos, Imre},
	month = oct,
	year = {2015}
}

@book{wilke_ggridges_2018,
	title = {ggridges: {Ridgeline} {Plots} in 'ggplot2'},
	url = {https://CRAN.R-project.org/package=ggridges},
	author = {Wilke, Claus O.},
	year = {2018}
}

@article{repro-guide,
	title = {Reproducibility in {Science}: {A} {Guide} to enhancing reproducibility in scientific results and writing},
	url = {http://ropensci.github.io/reproducibility-guide/}
}

@book{davey_introduction_2002-1,
	title = {Introduction to {Lattices} and {Order}},
	isbn = {978-0-521-78451-1},
	abstract = {This new edition of Introduction to Lattices and Order presents a radical reorganization and updating, though its primary aim is unchanged. The explosive development of theoretical computer science in recent years has, in particular, influenced the book's evolution: a fresh treatment of fixpoints testifies to this and Galois connections now feature prominently. An early presentation of concept analysis gives both a concrete foundation for the subsequent theory of complete lattices and a glimpse of a methodology for data analysis that is of commercial value in social science. Classroom experience has led to numerous pedagogical improvements and many new exercises have been added. As before, exposure to elementary abstract algebra and the notation of set theory are the only prerequisites, making the book suitable for advanced undergraduates and beginning graduate students. It will also be a valuable resource for anyone who meets ordered structures.},
	language = {English},
	publisher = {Cambridge University Press},
	author = {Davey, B. A. and Priestley, H. A.},
	month = apr,
	year = {2002},
	note = {Google-Books-ID: vVVTxeuiyvQC },
	keywords = {Mathematics / Algebra / Abstract, Mathematics / Algebra / General, Mathematics / Combinatorics, Mathematics / Discrete Mathematics, Mathematics / Logic}
}

@article{navarro_learning_2019,
	title = {Learning statistics with {R}: {A} tutorial for psychology students and other beginners. ({Version} 0.6.1)},
	url = {https://learningstatisticswithr.com/book/},
	urldate = {2019-06-07},
	author = {Navarro, Danielle},
	year = {2019}
}

@book{davey_when_2009,
	edition = {2},
	title = {When is a {Proof}?},
	publisher = {La Trobe University},
	author = {Davey, Brian A.},
	year = {2009}
}

@incollection{fidler_reproducibility_2018,
	edition = {Winter 2018},
	title = {Reproducibility of {Scientific} {Results}},
	abstract = {The terms “reproducibility crisis” and “replicationcrisis” gained currency in conversation and in print over thelast decade (e.g., Pashler \& Wagenmakers 2012), as disappointingresults emerged from large scale reproducibility projects in variousmedical, life and behavioural sciences (e.g., Open ScienceCollaboration, OSC 2015). In 2016, a poll conducted by the journalNature reported that more than half (52\%) of scientistssurveyed believed science was facing a “replicationcrisis” (Baker 2016). More recently, some authors have moved tomore positive terms for describing this episode in science; forexample, Vazire (2018) refers instead to a “credibilityrevolution” highlighting the improved methods and open sciencepractices it has motivated., The crisis often refers collectively to at least the following things:, The associated open science reform movement aims to rectify conditionsthat led to the crisis. This is done by promoting activities such asdata sharing and public pre-registration of studies, and by advocatingstricter editorial policies around statistical reporting includingpublishing replication studies and statistically non-significantresults., This review consists of four distinct parts. First, we look at theterm “reproducibility” and related terms like“repeatability” and “replication”, presentingsome definitions and conceptual discussion about the epistemicfunction of different types of replication studies. Second, wedescribe the meta-science research that has established andcharacterised the reproducibility crisis, including large scalereplication projects and surveys of questionable research practices invarious scientific communities. Third, we look at attempts to addressepistemological questions about the limitations of replication, andwhat value it holds for scientific inquiry and the accumulation ofknowledge. The fourth and final part describes some of the manyinitiatives the open science reform movement has proposed (and in manycases implemented) to improve reproducibility in science. In addition,we reflect there on the values and norms which those reforms embody,noting their relevance to the debate about the role of values in thephilosophy of science.},
	booktitle = {The {Stanford} {Encyclopedia} of {Philosophy}},
	publisher = {Metaphysics Research Lab, Stanford University},
	author = {Fidler, Fiona and Wilcox, John},
	editor = {Zalta, Edward N.},
	year = {2018}
}

@book{auburn_proof_2001,
	title = {Proof: {A} {Play}},
	isbn = {978-0-571-19997-6},
	shorttitle = {Proof},
	abstract = {Proof is the winner of the 2001 Pulitzer Prize for Drama.One of the most acclaimed plays of the 1999-2000 season, Proof is a work that explores the unknowability of love as much as it does the mysteries of science.It focuses on Catherine, a young woman who has spent years caring for her father, Robert, a brilliant mathematician in his youth who was later unable to function without her help. His death has brought into her midst both her sister, Claire, who wants to take Catherine back to New York with her, and Hal, a former student of Catherine's father who hopes to find some hint of Robert's genius among his incoherent scribblings. The passion that Hal feels for math both moves and angers Catherine, who, in her exhaustion, is torn between missing her father and resenting the great sacrifices she made for him. For Catherine has inherited at least a part of her father's brilliance -- and perhaps some of his instability as well. As she and Hal become attracted to each other, they push at the edges of each other's knowledge, considering not only the unpredictability of genius but also the human instinct toward love and trust.},
	language = {English},
	publisher = {Farrar, Straus and Giroux},
	author = {Auburn, David},
	month = mar,
	year = {2001},
	note = {Google-Books-ID: 6AUtQVhrY90C },
	keywords = {Drama / American / General}
}

@article{open_source_comprehensive_2019,
	title = {The {Comprehensive} {R} {Archive} {Network}},
	url = {https://cran.r-project.org/},
	urldate = {2019-06-01},
	author = {source, Open},
	year = {2019}
}

@techreport{marwick_packaging_2018,
	title = {Packaging data analytical work reproducibly using {R} (and friends)},
	url = {https://peerj.com/preprints/3192},
	abstract = {Computers are a central tool in the research process, enabling complex and large scale data analysis. As computer-based research has increased in complexity, so have the challenges of ensuring that this research is reproducible. To address this challenge, we review the concept of the research compendium as a solution for providing a standard and easily recognisable way for organising the digital materials of a research project to enable other researchers to inspect, reproduce, and extend the research. We investigate how the structure and tooling of software packages of the R programming language are being used to produce research compendia in a variety of disciplines. We also describe how software engineering tools and services are being used by researchers to streamline working with research compendia. Using real-world examples, we show how researchers can improve the reproducibility of their work using research compendia based on R packages and related tools.},
	language = {English},
	number = {e3192v2},
	urldate = {2019-05-29},
	institution = {PeerJ Inc.},
	author = {Marwick, Ben and Boettiger, Carl and Mullen, Lincoln},
	month = mar,
	year = {2018},
	doi = {10.7287/peerj.preprints.3192v2}
}

@article{marwick_tool-driven_2019,
	series = {preprint},
	title = {Tool-driven {Revolutions} in {Archaeological} {Science}},
	url = {https://osf.io/preprints/socarxiv/4nkxv/},
	doi = {10.31235/osf.io/4nkxv},
	abstract = {There is an argument in philosophy of science that revolutions in science are either ideas-driven or tool-driven. We explore this debate in light of recent efforts by many scientific disciplines to embrace methods to improve the reproducibility of their research. One of the most profound changes driven by this concern for reproducibility and transparency is from analysing data using tools dependent on pointing-and-clicking with a mouse in commercial software, to a new set of tools based on writing scripts in free and open source programming languages. We present bibliometric evidence for this change in the sciences generally and in archaeology in particular to test if the adoption of these new tools may be considered a revolution or some other process. We identify a citation effect for papers that use code. We discuss how computational approaches to improving reproducibility and transparency in archaeology are mediated and transformed by the use of code.},
	urldate = {2019-03-26},
	journal = {preprint},
	author = {Marwick, Ben and Schmidt, Sophie},
	month = jan,
	year = {2019}
}

@article{wilson_best_2014,
	title = {Best {Practices} for {Scientific} {Computing}},
	volume = {12},
	issn = {1545-7885},
	url = {https://dx.plos.org/10.1371/journal.pbio.1001745},
	doi = {10.1371/journal.pbio.1001745},
	language = {English},
	number = {1},
	urldate = {2018-11-10},
	journal = {PLoS Biology},
	author = {Wilson, Greg and Aruliah, D. A. and Brown, C. Titus and Chue Hong, Neil P. and Davis, Matt and Guy, Richard T. and Haddock, Steven H. D. and Huff, Kathryn D. and Mitchell, Ian M. and Plumbley, Mark D. and Waugh, Ben and White, Ethan P. and Wilson, Paul},
	editor = {Eisen, Jonathan A.},
	month = jan,
	year = {2014},
	keywords = {dumpsterfire, measured.},
	pages = {e1001745}
}

@article{wilson_good_2017,
	title = {Good enough practices in scientific computing},
	volume = {13},
	issn = {1553-7358},
	url = {http://dx.plos.org/10.1371/journal.pcbi.1005510},
	doi = {10.1371/journal.pcbi.1005510},
	language = {English},
	number = {6},
	urldate = {2018-11-17},
	journal = {PLOS Computational Biology},
	author = {Wilson, Greg and Bryan, Jennifer and Cranston, Karen and Kitzes, Justin and Nederbragt, Lex and Teal, Tracy K.},
	editor = {Ouellette, Francis},
	month = jun,
	year = {2017},
	keywords = {dumpsterfire, measured.},
	pages = {e1005510}
}

@book{sorensen2006lectures,
	title = {Lectures on the {Curry}-{Howard} isomorphism},
	volume = {149},
	publisher = {Elsevier},
	author = {Sørensen, Morten Heine and Urzyczyn, Pawel},
	year = {2006}
}

@article{devezerScientificDiscoveryModelcentric2019,
  title = {Scientific Discovery in a Model-Centric Framework: {{Reproducibility}}, Innovation, and Epistemic Diversity},
  shorttitle = {Scientific Discovery in a Model-Centric Framework},
  author = {Devezer, Berna and Nardin, Luis G. and Baumgaertner, Bert and Buzbas, Erkan Ozge},
  year = {2019},
  month = may,
  volume = {14},
  pages = {e0216125},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0216125},
  file = {/home/cantabile/Zotero/storage/IWGSS6CT/Devezer et al. - 2019 - Scientific discovery in a model-centric framework.pdf;/home/cantabile/Zotero/storage/HV729IMQ/article.html},
  journal = {PLOS ONE},
  keywords = {Markov models,Replication studies,Reproducibility,Scientists,Species diversity,Statistical data,Statistical theories,Stochastic processes},
  language = {en},
  number = {5}
}
